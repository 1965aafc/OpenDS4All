{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lecture_Notebook_Remote_New_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp",
        "colab_type": "text"
      },
      "source": [
        "# Lecture Notebook: Big Data and Graph Data\n",
        "\n",
        "Apache Spark is a big data engine that runs on compute clusters, including on the cloud.  This notebook is set up assuming that (1) Spark is running on a cloud server that is public and (2) we need to run the actual Python commands on that server, requiring us to put `%%spark` \"magic\" commands at the start of each cell.\n",
        "\n",
        "You may wish to look at this notebook first, without directly running it, until you understand how to launch your own Spark cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPw_ePbge5xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install libkrb5-dev\n",
        "!pip install sparkmagic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrFAyi5re65W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stbF4ttIQBUG",
        "colab_type": "text"
      },
      "source": [
        "The following line connects to Spark running remotely.  You will need to change the URL after the `-u` to connect to an active server. Check the instruction on how to set up EMR cluster on AWS if you are not aware of how to do it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifm9kBmus86c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%spark add -s my_session -l python -u http://ec2-18-213-115-72.compute-1.amazonaws.com:8998 \n",
        "# The above can connect to an EMR node running Spark + Livy, assuming the firewall is set to let anyone in"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgP5iO3vT2MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%spark delete -s my_session_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u6rwG-wQIX2",
        "colab_type": "text"
      },
      "source": [
        "## Example of Loading Sharded Data (Lecture Slides A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O02TOXqQLgt",
        "colab_type": "text"
      },
      "source": [
        "Some preliminaries:  **Every** cell in this notebook will need `%%spark` at the start so it runs on the remote machine with Spark instead of on the machine with Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDHoasDnQVLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "import json\n",
        "# import requests\n",
        "import urllib.request as urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWz1lghRfrgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmXoH1gmtD2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "# 10K records from linkedin\n",
        "# linked_in = requests.get('X')\n",
        "# If you have problems with the above, use urllib \n",
        "linked_in = urllib.urlopen('https://XXX/test_data_10000.json')\n",
        "\n",
        "# my_list = [json.loads(line) for line in linked_in.iter_lines()]\n",
        "# If you have problems with the the above, use urllib \n",
        "my_list = [json.loads(line) for line in linked_in.readlines()]\n",
        "len(my_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-Or-F-Qc5C",
        "colab_type": "text"
      },
      "source": [
        "## Load the list into Spark\n",
        "\n",
        "Spark needs to know the structure of the data in its dataframes, i.e., their schemas.  Since our JSON structure for LinkedIn is complex, we need to define the schema.\n",
        "\n",
        "There are some basic types:\n",
        "  * The table is a `StructType` with a list of fields (each row)\n",
        "  * Most fields, in our case, are `StringType`.\n",
        "  * We also have nested dictionary for the name, which is a `MapType` from `StringType` keys to `StringType` values.\n",
        "  * `skills` is an `ArrayType` since it's a list, and it contains `StringType`s.\n",
        "  * `also_view` is an array of structs.\n",
        "\n",
        "See Pyspark documentation on `StructType` and examples such as https://www.programcreek.com/python/example/104715/pyspark.sql.types.StructType."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN3NJaRuP8Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Spark requires that we define a schema for the LinkedIn data...\n",
        "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, MapType\n",
        "schema = StructType([\n",
        "        StructField(\"_id\", StringType(), True),\n",
        "        StructField(\"name\", MapType(StringType(), StringType()), True),\n",
        "        StructField(\"locality\", StringType(), True),\n",
        "        StructField(\"skills\", ArrayType(StringType()), True),\n",
        "        StructField(\"industry\", StringType(), True),\n",
        "        StructField(\"summary\", StringType(), True),\n",
        "        StructField(\"url\", StringType(), True),\n",
        "        StructField(\"also_view\", ArrayType(\\\n",
        "                    StructType([\\\n",
        "                      StructField(\"url\", StringType(), True),\\\n",
        "                      StructField(\"id\", StringType(), True)])\\\n",
        "                    ), True)\\\n",
        "         ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmF8gC6nP3VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "# Load the remote data as a list of dictionaries\n",
        "linked_df = sqlContext.createDataFrame(my_list, schema).\\\n",
        "  repartition('_id')\n",
        "\n",
        "linked_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8TnR04oJVoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "linked_df.rdd.getNumPartitions()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUo_4LfOiki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "linked_df.filter(linked_df.locality == 'United States')[['_id', 'name', 'locality']].show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj1E3-AJ1G_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "linked_df.select(\"_id\", \"locality\").show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e68gcPiaL5Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "### Clean out the list from memory\n",
        "my_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD9SE_v-1v0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "linked_df.createOrReplaceTempView('linked_in')\n",
        "sqlContext.sql('select * from linked_in').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yur38PPmVGt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "sqlContext.sql('select _id, name.given_name, name.family_name from linked_in').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcvsuNw9Tr6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "acro = udf(lambda x: ''.join([n[0] for n in x.split()]), StringType())\n",
        "\n",
        "linked_df.select(\"_id\", acro(\"locality\").alias(\"acronym\")).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMW4qFCQf4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "# Which industries are most popular?\n",
        "sqlContext.sql('select count(_id), industry '+\\\n",
        "               'from linked_in '+\\\n",
        "               'group by industry '+\\\n",
        "               'order by count(_id) desc').\\\n",
        "    show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrsSWsmI7pG",
        "colab_type": "text"
      },
      "source": [
        "## Graphs \n",
        "\n",
        "For the next set of examples, we will look at graph-structured data.  It turns out our LinkedIn dataset has a list of nodes (by int ID, but associated with the user ID we used in the linked_in table) and a list of edges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLkmsCn02FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "import urllib.request as urllib\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "#url = 'E'\n",
        "url = 'https://XXX/linkedin.edges.zip'\n",
        "filehandle, _ = urllib.urlretrieve(url)\n",
        "\n",
        "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
        "fname = zip_file_object.open('linkedin.edges')\n",
        "\n",
        "edges = []\n",
        "MAX = 2000000\n",
        "\n",
        "for link in fname:\n",
        "  edge = link.decode('utf-8').split(' ')\n",
        "  edges.append([int(edge[0]), int(edge[1])])\n",
        "  if len(edges) >= MAX:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10raoCWVJQXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "schema = StructType([\n",
        "        StructField(\"from\", IntegerType(), True),\n",
        "        StructField(\"to\", IntegerType(), True)\n",
        "         ])\n",
        "# Load the remote data as a list of dictionaries\n",
        "edges_df = sqlContext.createDataFrame(edges, schema)\n",
        "\n",
        "edges_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4WTTHohshi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "edges_df.createOrReplaceTempView('edges')\n",
        "sqlContext.sql('select from as id, count(to) as degree from edges group by from').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbXkhbzOk1ZK",
        "colab_type": "text"
      },
      "source": [
        "## Traversing the Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJ_BjL1kiQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Start with a subset of nodes\n",
        "start_nodes_df = edges_df[['from']].filter(edges_df['from'] < 100000).\\\n",
        "  select(col('from').alias('id')).drop_duplicates()\n",
        "\n",
        "start_nodes_df.show(5)\n",
        "\n",
        "# The neighbors require us to join\n",
        "# and we'll use Spark DataFrames syntax here\n",
        "neighbor_nodes_df = start_nodes_df.\\\n",
        "  join(edges_df, start_nodes_df.id == edges_df['from']).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_nodes_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpoPxuPQnmNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "edges_df[['from']].orderBy('from').drop_duplicates().show()\n",
        "\n",
        "edges_df.filter(edges_df['from'] == 698).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pnH4lKqNiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "neighbor_neighbor_nodes_df = neighbor_nodes_df.\\\n",
        "  join(edges_df, neighbor_nodes_df.id == edges_df['from']).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_neighbor_nodes_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvsJ2_LNsTbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "def iterate(df, depth):\n",
        "  df.createOrReplaceTempView('iter')\n",
        "\n",
        "  # Base case: direct connection\n",
        "  result = sqlContext.sql('select from, to, 1 as depth from iter')\n",
        "\n",
        "  for i in range(1, depth):\n",
        "    result.createOrReplaceTempView('result')\n",
        "    result = sqlContext.sql('select r1.from as from, r2.to as to, r1.depth+1 as depth  '\\\n",
        "                            'from result r1 join iter r2 '\\\n",
        "                            'on r1.to=r2.from')\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiE4BBMuwqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 1).orderBy('from','to').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63RGiC1euw-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 2).orderBy('from','to').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wG7vMrXuxFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 3).orderBy('from','to').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N1eHyunMccX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "# Clear list of edges from Python memory\n",
        "# to free up space\n",
        "edges = []\n",
        "\n",
        "\n",
        "### Now let's get the list of node IDs\n",
        "#url = 'N'\n",
        "url = 'https://XXX/linkedin.nodes.new.zip'\n",
        "nodehandle, _ = urllib.urlretrieve(url)\n",
        "\n",
        "zip_file_object = zipfile.ZipFile(nodehandle, 'r')\n",
        "fname = zip_file_object.open('linkedin.nodes.new')\n",
        "\n",
        "nodes = []\n",
        "MAX = 100000\n",
        "\n",
        "for node in fname:\n",
        "  node_tuple = node.split()\n",
        "  nodes.append([int(node_tuple[0]), str(node_tuple[1])])\n",
        "  if len(nodes) >= MAX:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCba0mxkikw",
        "colab_type": "text"
      },
      "source": [
        "## Joins in Spark, Beyond Graph Traversals\n",
        "\n",
        "What if we want to connect our edges to the people from our previous crawl?  Sadly the edges use int node IDs that don't correspond to the people dataframe.  But in fact the node data includes this information, so let's load and exploit that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wg-2C56pP_s",
        "colab_type": "text"
      },
      "source": [
        "Let's load the information about nodes, and their correspondence to the user ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3iZL-pWX8xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "schema = StructType([\n",
        "        StructField(\"nid\", IntegerType(), True),\n",
        "        StructField(\"user\", StringType(), True)\n",
        "         ])\n",
        "# Load the remote data as a list of dictionaries\n",
        "nodes_df = sqlContext.createDataFrame(nodes, schema)\n",
        "\n",
        "nodes_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVwMCIspVPc",
        "colab_type": "text"
      },
      "source": [
        "## Finding Friends, by ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18K1_i1jZ94B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "\n",
        "nodes_df.createOrReplaceTempView('nodes')\n",
        "edges_df.createOrReplaceTempView('edges')\n",
        "\n",
        "friends_df = \\\n",
        "sqlContext.sql('select n1.user, n2.user as friend ' +\\\n",
        "               'from (nodes n1 join edges e on n1.nid = e.from) join nodes n2 on e.to = n2.nid')\n",
        "\n",
        "friends_df.show(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp5VkLumpYmT",
        "colab_type": "text"
      },
      "source": [
        "## Connecting Friends to Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9PkrJobXY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%spark\n",
        "friends_df.createOrReplaceTempView('friends')\n",
        "\n",
        "sqlContext.sql('select u1.name.given_name as user, u2.name.given_name as friend '+\\\n",
        "               'from (linked_in u1 join friends on u1._id = user) join linked_in u2 on u2._id = friend').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNUlRh76jJps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}