{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_sol_Local.ipynb","provenance":[{"file_id":"1V4kiujaAnm3hHMomSK1eDz5jQjdKxmWU","timestamp":1571083042642}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qVMCjEV1IEC7","colab_type":"text"},"source":["#Homework: Spark SQL\n","\n","In this homework you will gain a mastery of using Spark SQL. The homework can be run locally or on an EMR cluster.  The current version is for running locally.  \n","\n","The goal of the homework will be to create a training dataset for a Random Forest Machine learning model. The training data set will contain the monthly number of employees hired by companies in `linkedin.json` and their corresponding closing stock prices over a 10+ year period (1970-2018 `stock_prices.csv`). We will try and predict, based on this data, if the company will have a positive or negative growth in stock in the first quarter of the next year. Who's ready to make some money?\n","\n","## Notes\n","Before we begin here are some important notes to keep in mind,\n","\n","1. You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n","\n","2. There are portions of this homework that are _very_ challenging. \n"]},{"cell_type":"code","metadata":{"id":"pvkEbVaaAQ1e","colab_type":"code","colab":{}},"source":["%%capture\n","!apt update\n","!apt install gcc python-dev libkrb5-dev\n","!pip install sparkmagic\n","!pip install pyspark"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAvEMpzqqHeY","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","from pyspark.sql import SQLContext\n","import json\n","import urllib.request\n","\n","from datetime import datetime\n","\n","try:\n","    if(spark == None):\n","        spark = SparkSession.builder.appName('Graphs').getOrCreate()\n","        sqlContext=SQLContext(spark)\n","        \n","except NameError:\n","    spark = SparkSession.builder.appName('Graphs').getOrCreate()\n","    sqlContext=SQLContext(spark)\n","        \n","from pyspark.sql.types import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nf_ADEXnIK0b","colab_type":"text"},"source":["## Step 1: Data Cleaning and Shaping\n","\n","The data you will use is stored in an S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). \n","\n","### Step 1.1: The Stupendous Schema\n","\n","When loading data, Spark will try to infer the structure. This process is faulty because it will sometimes infer the type incorrectly. JSON documents, like the one we will use, can have nested types, such as: arrays, arrays of dictionaries, dictionaries of dictionaries, etc. Spark's ability to determine these nested types is not reliable, thus you will define a schema for `linkedin.json`.\n","\n","A schema is a description of the structure of data. You will be defining an explicit schema for `linkedin.json`. In Spark, schemas are defined using a `StructType` object. This is a collection of data types, termed `StructField`s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object,\n","\n","\n","```\n","{\n"," \"student_name\": \"Leonardo Murri\",\n"," \"GPA\": 1.4,\n"," \"courses\": [\n","    {\"department\": \"Computer and Information Science\",\n","     \"course_id\": \"CIS 545\",\n","     \"semester\": \"Fall 2018\"},\n","    {\"department\": \"Computer and Information Science\",\n","     \"course_id\": \"CIS 520\",\n","     \"semester\": \"Fall 2018\"},\n","    {\"department\": \"Electrical and Systems Engineering\",\n","     \"course_id\": \"ESE 650\",\n","     \"semester\": \"Spring 2018\"}\n"," ],\n"," \"grad_year\": 2019\n"," }\n","```\n","\n","We would define its schema as follows,\n","\n","```       \n","schema = StructType([\n","           StructField(\"student_name\", StringType(), nullable=True),\n","           StructField(\"GPA\", FloatType(), nullable=True),\n","           StructField(\"courses\", ArrayType(\n","                StructType([\n","                  StructField(\"department\", StringType(), nullable=True),\n","                  StructField(\"course_id\", StringType(), nullable=True),\n","                  StructField(\"semester\", StringType(), nullable=True)\n","                ])\n","           ), nullable=True),\n","           StructField(\"grad_year\", IntegerType(), nullable=True)\n","         ])\n","```\n","\n","\n","Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `linkedin.json`. \n","\n","_Note_: In `linkedin.json` the field `specilities` is spelled incorrectly. This is **not** a typo. \n"]},{"cell_type":"code","metadata":{"id":"pL-Ps4KWIJ9e","colab_type":"code","colab":{}},"source":["# TODO: Define [linkedin.json] schema\n","# YOUR CODE HERE\n","### BEGIN SOLUTION\n","schema = StructType([\n","    StructField(\"_id\", StringType(), nullable=True),\n","    StructField(\"education\", ArrayType(\n","      StructType([\n","          StructField(\"start\", StringType(), nullable=True),\n","          StructField(\"major\", StringType(), nullable=True),\n","          StructField(\"end\", StringType(), nullable=True),\n","          StructField(\"name\", StringType(), nullable=True),\n","          StructField(\"desc\", StringType(), nullable=True),\n","          StructField(\"degree\", StringType(), nullable=True)\n","      ])\n","    ), nullable=True),\n","    StructField(\"group\", StructType([\n","          StructField(\"affilition\", ArrayType(StringType()), nullable=True),\n","          StructField(\"member\", StringType(), nullable=True)\n","    ]), nullable=True),\n","    StructField(\"name\", StructType([\n","        StructField(\"family_name\", StringType(), nullable=True),\n","        StructField(\"given_name\", StringType(), nullable=True)\n","    ]), nullable=True),\n","    StructField(\"locality\", StringType(), nullable=True),\n","    StructField(\"skills\", ArrayType(StringType()), nullable=True),\n","    StructField(\"industry\", StringType(), nullable=True),\n","    StructField(\"interval\", IntegerType(), nullable=True),\n","    StructField(\"experience\", ArrayType(\n","      StructType([\n","          StructField(\"org\", StringType(), nullable=True),\n","          StructField(\"title\", StringType(), nullable=True),\n","          StructField(\"end\", StringType(), nullable=True),\n","          StructField(\"start\", StringType(), nullable=True),\n","          StructField(\"desc\", StringType(), nullable=True)\n","      ])), nullable=True),\n","    StructField(\"summary\", StringType(), nullable=True),\n","    StructField(\"interests\", StringType(), nullable=True),\n","    StructField(\"overview_html\", StringType(), nullable=True),\n","    StructField(\"specilities\", StringType(), nullable=True),\n","    StructField(\"homepage\", ArrayType(StringType()), nullable=True),\n","    StructField(\"honors\", ArrayType(StringType()), nullable=True),\n","    StructField(\"url\", StringType(), nullable=True),\n","    StructField(\"also_view\", ArrayType(\n","      StructType([\n","          StructField(\"id\", StringType(), nullable=True),\n","          StructField(\"url\", StringType(), nullable=True)\n","      ])\n","    ), nullable=True),\n","    StructField(\"events\", ArrayType(\n","      StructType([\n","          StructField(\"from\", StringType(), nullable=True),\n","          StructField(\"to\", StringType(), nullable=True),\n","          StructField(\"title1\", StringType(), nullable=True),\n","          StructField(\"start\", IntegerType(), nullable=True),\n","          StructField(\"title2\", StringType(), nullable=True),\n","          StructField(\"end\", IntegerType(), nullable=True)\n","      ])), nullable=True)\n","])\n","  ### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Su604X9ggc2","colab_type":"text"},"source":["### Step 1.2: The Laudable Loading\n","\n","Load the `linkedin.json` dataset into a Spark dataframe (sdf) called `raw_data_sdf`. If you have constructed `schema` correctly `spark.read.json()` will read in the dataset. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"EKFsXC0l31lU","colab_type":"code","colab":{}},"source":["import urllib\n","#url = 'X'\n","url = 'https://upenn-bigdataanalytics.s3.amazonaws.com/linkedin.zip'\n","filehandle, _ = urllib.request.urlretrieve(url,filename='local.zip')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-FoRLQF6l6v","colab_type":"code","colab":{}},"source":["import zipfile\n","zip_file_object = zipfile.ZipFile(filehandle, 'r')\n","zip_file_object.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jCDWXJAQZ6b","colab_type":"code","colab":{}},"source":["!rm -f local.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ji-KW2sAiB6r","colab_type":"code","colab":{}},"source":["raw_data_sdf = spark.read.json('linkedin.json', schema=schema)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMVCVotcE1wv","colab_type":"text"},"source":["The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"NJSVWeGiEO5c","colab_type":"code","outputId":"9b6d4a1d-e718-4129-8977-05c94fcf6e57","executionInfo":{"status":"ok","timestamp":1576003202416,"user_tz":300,"elapsed":584627,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["# Create SQL-accesible table\n","raw_data_sdf.createOrReplaceTempView(\"raw_data\")\n","\n","# Declare SQL query to be excecuted\n","query = '''SELECT * \n","           FROM raw_data'''\n","\n","# Save the output sdf of spark.sql() as answer_sdf\n","answer_sdf = spark.sql(query)\n","\n","# Display the first 10 rows\n","answer_sdf.show(10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n","|               _id|           education|               group|                name|            locality|              skills|            industry|interval|          experience|             summary|           interests|       overview_html|         specilities|homepage|              honors|                 url|           also_view|              events|\n","+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n","|       in-00000001|                null|                null|[Mazalu MBA, Dr C...|       United States|[Key Account Deve...|     Medical Devices|    null|                null|SALES MANAGEMENT ...|                null|                null|                null|    null|                null|http://www.linked...|[[pub-krisa-drost...|                null|\n","|          in-00001|[[2008, Economics...|[[ASMALLWORLD.net...|     [Forslund, Ann]|Antwerp Area, Bel...|[Molecular Biolog...|     Pharmaceuticals|      20|[[Johnson and Joh...|Ph.D. scientist w...|                null|<dl id=\"overview\"...|Biomarkers in Onc...|    null|                null|http://be.linkedi...|[[pub-peter-king-...|[[Sahlgrenska Uni...|\n","|              null|                null|                null|                null|                null|                null|                null|    null|                null|                null|                null|                null|                null|    null|                null|                null|                null|                null|\n","|  in-000montgomery|                null|[[Big Data, Low L...|   [Kilimann, Edric]|San Francisco Bay...|                null|Information Techn...|       5|[[<Online Recruit...|OBJECTIVE<Primary...|                null|                null|                null|    null|                null|http://www.linked...|[[pub-david-brigh...|[[<Employee Benef...|\n","|in-000vijaychauhan|[[1988,, 1989, Ec...|[[AeSI Alumni Ass...|[Chauhan, PMP, Vi...| Chennai Area, India|[Program Manageme...|Aviation & Aerospace|    null|                null|Experience in Avi...|Literature, Philo...|                null|                null|    null|                null|http://in.linkedi...|[[in-sandeepraghu...|                null|\n","|  in-001adambutler|[[1991, Product D...|                null|     [Adam, Butler,]|Brighton, United ...|[Digital Strategy...|Marketing and Adv...|      16|[[Brand New Music...|Integrating creat...|travelling,the se...|                null|A passion for Bra...|    null|                null|http://uk.linkedi...|[[in-paulbeier, h...|[[Tigerprint, WHS...|\n","|      in-001monica|[[2000, Economics...|[[Canadian Market...|    [Andrus, Monica]|Toronto, Canada Area|                null|Nonprofit Organiz...|      20|[[Canadian MedicA...|                null|                null|                null|                null|    null|                null|http://ca.linkedi...|                  []|[[CMAF, CMAF, Bus...|\n","| in-001neilpeacock|                null|                null|     [Peacock, Neil]|      United Kingdom|[DLP, Managed Ser...|Computer & Networ...|      31|[[Complete IT Sys...|Currently Trainin...|                null|                null|                null|    null|                null|http://uk.linkedi...|[[pub-dilan-hindo...|[[Dawson Rentals ...|\n","|          in-00666|                null|                null|[BOLUKBAS, PMP, M...|              Turkey|[Project Manageme...|    Telekomünikasyon|    null|                null|                null|                null|                null|                null|    null|                null|http://tr.linkedi...|[[pub-dilek-karap...|                null|\n","|       in-00789123|[[2009, Business ...|[[CFA Institute C...| [Hoffmann, Lynette]|        South Africa|[Predictive Analy...|  Telecommunications|      12|[[Ericsson Region...|A Marketing profe...|Marketing and sta...|                null|                null|    null|[Recognition Awar...|http://za.linkedi...|[[pub-mashudu-tho...|[[Eskom, ESKOM Di...|\n","+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svOO4iLPist4","colab_type":"text"},"source":["### Step 1.3: The Extravagent Extraction\n","\n","In our training model, we are interested in when individuals began working at a company.  From creating the schema, you should notice that the collection of companies inviduals worked at are contained in the `experience` field as an array of dictionaries. You should use the `org` for the company name and `start` for the start date. Here is an example of an `experience` field,\n","\n","```\n","{\n","   \"experience\": [\n","     {\n","        \"org\": \"The Walt Disney Company\", \n","        \"title\" : \"Mickey Mouse\",\n","        \"end\" : \"Present\",\n","        \"start\": \"November 1928\",\n","        \"desc\": \"Sailed a boat.\"\n","     },\n","     {\n","        \"org\": \"Walt Disney World Resort\",\n","        \"title\": \"Mickey Mouse Mascot\",\n","        \"start\": \"January 2005\",\n","        \"desc\": \"Took pictures with kids.\"\n","     }\n","   ]\n","}\n","```\n","\n","Your task is to extract each pair of company and start date from these arrays. In Spark, this is known as \"exploding\" a row. An explode will seperate the elements of an array into multiple rows.\n","\n","Create an sdf called `raw_start_dates_sdf` that contains the company and start date for every experience of every individual in `raw_data_sdf`. Drop any row that contains a `null` in either column with `dropna()`. You can sort the elements however you wish (you don't need to if you don't want to). The sdf should look as follows:\n","\n","```\n","+--------------------------+---------------+\n","|org                       |start_date     |\n","+--------------------------+---------------+\n","|Walt Disney World Resort  |January 2005   | \n","|The Walt Disney Company   |November 1928  |\n","|...                       |...            |\n","+--------------------------+---------------+\n","```\n","\n","_Hint_: You may want to do two seperate explodes for `org` and `start`. In an explode, the position of the element in the array can be extracted as well, and used to merge two seperate explodes. Reference the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html).\n","\n","_Note_: Some of the entires in `org` are \"weird\", i.e. made up of non-english letters and characters. Keep them. **DO NOT** edit any name in the original dataframe unless we specify. **DO NOT** drop any row unless there is a `null` value as stated before. This goes for the rest of the homework as well, unless otherwise specified."]},{"cell_type":"code","metadata":{"id":"Kt16tyP0klQX","colab_type":"code","colab":{}},"source":["# TODO: Create [raw_start_dates_sdf]\n","\n","  ##YOUR ANSWER HERE\n","  ### BEGIN SOLUTION\n","\n","# POSEXPLODE() will explode a specified column of an sdf and return two rows\n","# corresponding to the index of an element in an array \"pos\" and the element\n","# itself.\n","\n","# Explode the org array in experience\n","query = '''SELECT _id AS id,\n","                  POSEXPLODE(experience.org) AS (pos, org)\n","           FROM raw_data'''\n","\n","# Save as a SQL-accesible table called \"orgs\"\n","spark.sql(query).dropna().createOrReplaceTempView(\"orgs\")\n","\n","# Explode the start_date array in experience\n","query = '''SELECT _id AS id,\n","                  POSEXPLODE(experience.start) AS (pos, start_date)\n","           FROM raw_data'''\n","spark.sql(query).dropna().createOrReplaceTempView(\"start_dates\")\n","\n","# Join each explode based on a person's id and the index of each element in the\n","# original array\n","query = '''SELECT o.org AS org,\n","                  s.start_date AS start_date\n","           FROM orgs AS o\n","           JOIN start_dates AS s\n","           ON o.id = s.id AND o.pos = s.pos'''\n","\n","# Define and save raw_start_dates_sdf\n","raw_start_dates_sdf = spark.sql(query)\n","raw_start_dates_sdf.createOrReplaceTempView(\"raw_start_dates\")\n","  ### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-a3xLDEYCcwl","colab_type":"code","outputId":"8f2baf31-2bd8-4806-add6-8c8d2dc64ad9","executionInfo":{"status":"ok","timestamp":1576003302872,"user_tz":300,"elapsed":685062,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["raw_start_dates_sdf.show(4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+--------------+\n","|                 org|    start_date|\n","+--------------------+--------------+\n","|             Spot.us|September 2009|\n","|Pfizer Ltd. (WPO ...|          2005|\n","|              ALSTOM|          2008|\n","|DSM i-Nutrition B...|  January 2009|\n","+--------------------+--------------+\n","only showing top 4 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zSbb0t-d-VFt","colab_type":"text"},"source":["### Step 1.4: The Fortuitous Formatting\n","\n","There are two issues with the values in our `date` column. First, the values are saved as strings, not datetime types. This keeps us from running functions such as `ORDER BY` or `GROUP BY` on common months or years. Second, some values do not have both month and year information or are in other languages. Your task is to filter out and clean the `date` column. We are interested in only those rows that have date in the following format \"(month_name) (year)\", e.g. \"October 2010\".\n","\n","Create an sdf called `filtered_start_dates_sdf` from `raw_start_dates_sdf` with the `date` column filtered in the manner above. Keep only those rows with a start date between January 2000 to December 2011, inclusive. Ensure that any dates that are not in our desired format are ommitted. Drop any row that contains a `null` in either column. The format of the sdf is shown below:\n","```\n","+--------------------------+---------------+\n","|org                       |start_date     |\n","+--------------------------+---------------+\n","|Walt Disney World Resort  |2005-01-01     | \n","|...                       |...            |\n","+--------------------------+---------------+\n","```\n","_Hint_: Refer to the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) to format the `date` column. In Spark SQL the date format we are interested in is `\"MMM y\"`.\n","\n","_Note_: Spark will return the date in the format above, with the day as `01`. This is ok, since we are interested in the month and year each individual began working and all dates will have `01` as their day."]},{"cell_type":"code","metadata":{"id":"eelgTtOc_MBM","colab_type":"code","colab":{}},"source":["# TODO: Create [filtered_start_dates_sdf]\n","\n","## YOUR ANSWER HERE\n","### BEGIN SOLUTION\n","# TO_DATE() will convert a string to a datetime object. The string's format,\n","# i.e. \"MMM y\" must be provided. Any string that does not have the specified\n","# format will be returned as null.\n","\n","# Use TO_DATE() to convert the start_date column from a string to a datetime\n","# object. Keep only dates that are between January 2000 and December 2011,\n","# inclusive.\n","query = '''SELECT org,\n","                  TO_DATE(start_date, \"MMM y\") AS start_date\n","           FROM raw_start_dates\n","           WHERE TO_DATE(start_date, \"MMM y\") >= \"2000-01-01\"\n","                 AND TO_DATE(start_date, \"MMM y\") <= \"2011-12-01\"'''\n","\n","# Define and save filtered_start_dates_sdf\n","filtered_start_dates_sdf = spark.sql(query)\n","filtered_start_dates_sdf.createOrReplaceTempView(\"filtered_start_dates\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXYYQn2_GYwZ","colab_type":"text"},"source":["### Step 1.5 The Gregarious Grouping\n","\n","We now want to collect the number of individuals that started in the same month and year for each company. Create an sdf called `start_dates_sdf` that has the total number of employees who began working at the same company on the same start date. The format of the sdf is shown below:\n","\n","```\n","+--------------------------+---------------+---------------+\n","|org                       |start_date     |num_employees  |\n","+--------------------------+---------------+---------------+\n","|Walt Disney World Resort  |2005-01-01     |1              |\n","|...                       |...            |...            |\n","+--------------------------+---------------+---------------+\n","```"]},{"cell_type":"code","metadata":{"id":"CxVIyc1CHooV","colab_type":"code","colab":{}},"source":["# TODO: Create [start_dates_sdf]\n","\n","## YOUR ANSWER HERE\n","### BEGIN SOLUTION\n","# GROUP BY on org and start_date, in that order.\n","query = '''SELECT org,\n","                  start_date,\n","                  COUNT(*) AS num_employees\n","           FROM filtered_start_dates\n","           GROUP BY org, start_date'''\n","\n","# Define and save start_dates_sdf\n","start_dates_sdf = spark.sql(query)\n","start_dates_sdf.createOrReplaceTempView(\"start_dates\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYScM3FwJnUz","colab_type":"text"},"source":["## Step 2: Hiring Trends Analysis\n","\n","Now we will analyze `start_dates_sdf` to find monthly and annual hiring trends.\n","\n","### Step 2.1: The Marvelous Months\n","\n","Your task is to answer the question: \"On average, what month do most employees start working?\" Create an sdf called `monthly_hires_sdf` which contains the total number of employees that started working on a specific month, at any company and on any year. The `month` column should be of type `int`, i.e. 1-12. The format of the sdf is shown below:\n","\n","```\n","+---------------+---------------+\n","|month          |num_employees  |\n","+---------------+---------------+\n","|1              |...            |\n","|2              |...            |\n","|3              |...            |\n","|...            |...            |\n","+---------------+---------------+\n","```\n","\n","Find the month in which the most employees start working and save its number as an integer to the variable `most_common_month`.\n","\n","_Hint_: Be careful. The start dates we have right now have both month and year. We only want the common months. See if you can find something in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) that will help you do this."]},{"cell_type":"code","metadata":{"id":"vLTmvD9WNQH3","colab_type":"code","colab":{}},"source":["# TODO: Create [monthly_hire_sdf] and find the most common month people were\n","# hired. Save its number as an integer to [most_common_month]\n","\n","## YOUR ANSWER HERE\n","### BEGIN SOLUTION\n","# MONTH() will return the month for any datetime object.\n","\n","# GROUP BY on the month of start_date using MONTH(). Sum the num_employees\n","# column to find the total number of employees that started on that month\n","query = '''SELECT MONTH(start_date) AS month,\n","                  SUM(num_employees) AS num\n","           FROM start_dates\n","           GROUP BY MONTH(start_date)'''\n","\n","# Define and save monthly_hires_sdf\n","monthly_hires_sdf = spark.sql(query)\n","monthly_hires_sdf.createOrReplaceTempView(\"monthly_hires\")\n","\n","most_common_month = 1\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUtVHRcMUoWp","colab_type":"text"},"source":["### Step 2.2: The Preposterous Percentages\n","\n","The next question we will answer is \"What is the percentage change in hires between 2010 and 2011 for each company?\" Create an sdf called `percentage_change_sdf` that has the percentage change between 2010 and 2011 for each company. The sdf should look as follows:\n","\n","```\n","+---------------------------+--------------------+\n","|org                        |percentage_change   |\n","+---------------------------+--------------------+\n","|Walt Disney World Resort   |12.3                |\n","|...                        |...                 |\n","+---------------------------+--------------------+\n","```\n","\n","_Note_: A percentage change can be positive or negative depending \n","on the difference between the two years.The formula for percent change is given below,\n","\n","$$\\text{% change} = \\frac{P_f-P_i}{P_f} \\times 100$$\n","\n","Here, $P_f$ is the final element (in this case the number of hires in 2011) and $P_i$ is initial element (the number of hires in 2010).\n","\n","_Hint_: This is a **difficult** question. We recommend using a combination of `GROUP BY` and `JOIN`. Keep in mind that operations between columns in SQL dataframes are often easier than those between rows. "]},{"cell_type":"code","metadata":{"id":"_AhhfLXpWq7y","colab_type":"code","colab":{}},"source":["# TODO: Create [percentage_change_sdf]\n","\n","## YOUR ANSWER HERE\n","### BEGIN SOLUTION\n","# YEAR() will return the year of a datetime object.\n","\n","# The column percentage_change is calculated by doing a JOIN. We join an sdf\n","# \"y1\" created by doing a GROUP BY on start_dates by org and the year of\n","# start_date where the year is 2010. We sum the number of employees. That sdf is\n","# joined with a similar sdf, \"y2\", but for the case when year is 2011. Then,\n","# using the formula in the description above we find the percentage_change of\n","# the two sum columns.\n","query = '''SELECT y1.org AS org,\n","                  (y2.num_employees-y1.num_employees)/(y2.num_employees)*100\n","                      AS percentage_change\n","           FROM (SELECT org,\n","                        YEAR(start_date) AS year,\n","                        SUM(num_employees) AS num_employees\n","                 FROM start_dates\n","                 WHERE YEAR(start_date) = 2010\n","                 GROUP BY org, YEAR(start_date)) AS y1\n","           JOIN (SELECT org,\n","                        YEAR(start_date) AS year,\n","                        SUM(num_employees) AS num_employees\n","                 FROM start_dates\n","                 WHERE YEAR(start_date) = 2011\n","                 GROUP BY org, YEAR(start_date)) AS y2\n","           ON y1.org = y2.org'''\n","\n","# Define and save percentage_change_sdf\n","percentage_change_sdf = spark.sql(query)\n","percentage_change_sdf.createOrReplaceTempView(\"percentage_change\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkF2RfLSXO0u","colab_type":"text"},"source":["## Step 3: Formatting the Training Data\n","\n","\n","Our overaching goal is to train a machine learning (ML) model that will use the monthly hiring trends of a company to predict a positive or negative gain in the company's stock in the first quarter of the following year. A ML model is trained on a set of observations. Each observation contains a set of features, `X`, and a label, `y`. The goal of the ML model is to create a function that takes any `X` as an input and outputs a predicted `y`. \n","\n","The machine learning model we will use is a [Random Forest Classifier](https://builtin.com/data-science/random-forest-algorithm). Each observation we will pass in will have 24 features (columns). These are the number of people hired from Jan to Dec and the company stock price on the last day of each month. The label will be the direction of the company's stock percentage change (positive, `1`, or negative, `-1`) in the first quarter of the following year. Each observation will correspond to a specified company's trends on a specified year. The format of our final training sdf is shown below. The first 26 columns define our observations, `X`, and the last column the label, `y`.\n","```\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |1            |\n","|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |-1           |\n","|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","```\n","\n","_Note_: We will use the first three letters of each month in naming, i.e. `jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec`\n","\n","\n","\n","### Step 3.1: The Harmonious Hires\n","\n","Your first task is to create the first half of the training table, i.e. the `jan_hired` through `dec_hired` columns. This will involve reshaping `start_dates_sdf`. Currently, `start_dates_sdf` has columns `org`, `start_date`, and `num_employees`. We want to group the rows together based on common `org` and years and create new columns for the number of employees that started working in each month of that year.\n","\n","Create an sdf called `raw_hirings_for_training_sdf` that has for a single company and a single year, the number of hires in Jan through Dec, and the total number of hires that year. Note that for each company you will have several rows corresponding to years between 2000 and 2011. It is ok if for a given company you don't have a given year. However, ensure that for a given company and given year, each month column has an entry, i.e. if no one was hired the value should be `0`. The format of the sdf is shown below: \n","```\n","+----+-----+----------+---------+----------+----------+\n","|org |year |jan_hired |   ...   |dec_hired |total_num |\n","+----+-----+----------+---------+----------+----------+\n","|IBM |2008 |...       |   ...   |...       |...       |\n","|IBM |2009 |...       |   ...   |...       |...       |\n","|... |...  |...       |   ...   |...       |...       |\n","+----+-----+----------+---------+----------+----------+\n","```\n","_Hint_: This is a **difficult** question. The tricky part is creating the additional columns of monthly hires, specifically when there are missing dates. In our dataset, if a company did not hire anybody in a given date, it will not appear in `start_dates_sdf`. We suggest you look into `CASE` and `WHEN` statements in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html)."]},{"cell_type":"code","metadata":{"id":"btp2wboHqg2J","colab_type":"code","colab":{}},"source":["# TODO: Create [raw_hire_train_sdf]\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# CASE() statements are SQL's equivalent of if else statements. WHEN a CASE is\n","# true THEN we define a function. ELSE we do another function and then END the\n","# statement.\n","\n","# The query is a GROUP BY. We group data based on the same company and year, as\n","# in the previous step. We then do a CASE statement. This will seperate out the\n","# sets of data corresponding to the same month using MONTH() in the WHEN clause.\n","# If we have a piece of data, it will be the number of employees that started\n","# working at a given company on a given year and a given month and we will save\n","# it with a corresponding column name. If there is no piece of data here, as per\n","# the question, we need to add a 0. This is the ELSE clause. Lastly, we do a\n","# SUM() to find total_num\n","query = '''SELECT org, \n","                  YEAR(start_date) AS year,\n","                  MAX((CASE WHEN (MONTH(start_date) == 1) \n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS jan_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 2)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS feb_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 3)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS mar_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 4)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS apr_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 5)\n","                                  THEN num_employees \n","                                  ELSE 0 END)) AS may_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 6)\n","                                  THEN num_employees\n","                                   ELSE 0 END)) AS jun_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 7)\n","                                  THEN num_employees \n","                                  ELSE 0 END)) AS jul_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 8)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS aug_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 9)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS sep_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 10)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS oct_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 11)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS nov_hired,\n","                  MAX((CASE WHEN (MONTH(start_date) == 12)\n","                                  THEN num_employees\n","                                  ELSE 0 END)) AS dec_hired,\n","                  SUM(num_employees) AS total_num\n","           FROM start_dates\n","           GROUP BY org, YEAR(start_date)'''\n","\n","# Define and save raw_hire_train_sdf\n","raw_hire_train_sdf = spark.sql(query)\n","raw_hire_train_sdf.createOrReplaceTempView(\"raw_hire_train\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkXyet6rrczK","colab_type":"text"},"source":["### Step 3.2: The Formidable Filters\n","\n","Create an sdf called `hire_train_sdf` that contains all the observations in `raw_hire_train_sdf` with `total_num` greater than or equal to 500. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+----------+\n","|org |year |jan_hired |   ...   |dec_hired |total_num |\n","+----+-----+----------+---------+----------+----------+\n","|IBM |2008 |...       |   ...   |...       |...       |\n","|IBM |2009 |...       |   ...   |...       |...       |\n","|... |...  |...       |   ...   |...       |...       |\n","+----+-----+----------+---------+----------+----------+\n","```\n"]},{"cell_type":"code","metadata":{"id":"dCH4mbNcshq9","colab_type":"code","colab":{}},"source":["# TODO: Create [hire_train_sdf]\n","\n","##YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# Keep all rows where total_num >= 500\n","query = '''SELECT *\n","           FROM raw_hire_train\n","           WHERE total_num >= 500'''\n","\n","# Define and save hire_train_sdf\n","hire_train_sdf = spark.sql(query)\n","hire_train_sdf.createOrReplaceTempView(\"hire_train\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MN4ik70Hta01","colab_type":"text"},"source":["### Step 3.3: The Stupendous Stocks\n","\n","Now we are ready for the stock data. The stock data we will use is saved in the same S3 bucket as `linkedin.json`. Load the data into the EMR cluster. Run the cell below. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"TBVqrxmYFfWV","colab_type":"code","outputId":"d1250121-ac53-4034-fb59-54d052bb5a78","executionInfo":{"status":"ok","timestamp":1576004615257,"user_tz":300,"elapsed":48980,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"APv4BxKw643q","colab_type":"code","outputId":"7123e559-bc89-423e-e98d-790136291779","executionInfo":{"status":"ok","timestamp":1576004674447,"user_tz":300,"elapsed":2241,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["# Load stock data\n","raw_stocks_sdf = spark.read.format(\"csv\") \\\n","              .option(\"header\", \"true\") \\\n","              .load(\"./gdrive/My Drive/Colab Notebooks/stock_prices.csv\")\n","              # .load(\"s3a://545emr/stock_prices.csv\")\n","\n","# Creates SQL-accesible table\n","raw_stocks_sdf.createOrReplaceTempView('raw_stocks')\n","\n","# Display the first 10 rows\n","query = '''SELECT *\n","           FROM raw_stocks'''\n","spark.sql(query).show(10)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+------+-----------------+----------+\n","|ticker|    closing_price|      date|\n","+------+-----------------+----------+\n","|   AHH| 8.49315452575684|2013-05-08|\n","|   AHH| 8.47115135192871|2013-05-09|\n","|   AHH| 8.50782203674316|2013-05-10|\n","|   AHH| 8.54449367523193|2013-05-13|\n","|   AHH|8.456483840942381|2013-05-14|\n","|   AHH| 8.50782203674316|2013-05-15|\n","|   AHH| 8.61050128936768|2013-05-16|\n","|   AHH|8.625171661376951|2013-05-17|\n","|   AHH| 8.60316944122314|2013-05-20|\n","|   AHH|8.676511764526369|2013-05-21|\n","+------+-----------------+----------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JUCdr3zDUAFH","colab_type":"text"},"source":["Run the cell below to see the types of the columns in our data frame. These are not correct. We could have defined a schema when reading in data but we will handle this issue in another manner. You will do this in Step 3.4.2."]},{"cell_type":"code","metadata":{"id":"oNTGEfxsisqs","colab_type":"code","outputId":"02be64cf-f151-4226-beba-327c80c95e62","executionInfo":{"status":"ok","timestamp":1576004690927,"user_tz":300,"elapsed":576,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Print types of SDF\n","raw_stocks_sdf.dtypes"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('ticker', 'string'), ('closing_price', 'string'), ('date', 'string')]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"0DdnoFkP7mxz","colab_type":"text"},"source":["### Step 3.4 The Clairvoyant Cleaning\n","\n","We now want to format the stock data set into the second half of the training table. We will then merge it with `hire_train` based off the common `org` and `year` fields.\n","\n","#### Step 3.4.1 The Ubiquitous UDF\n","\n","The companies in our stock dataset are defined by their stock tickers. Thus, we would not be able to merge it with the `org` field in `hire_train_sdf`. We must convert them to that format. Often times when using Spark, there may not be a built-in SQL function that can do the operation we desired. Instead, we can create one on our own with a user-defined function (udf).\n","\n","A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `TICKER_TO_NAME()` that will convert the ticker field in `raw_stocks` to the company's name. This will be done using the provided `ticker_to_name_dict` dictionary. We are only interested in the companies in that dictionary.\n","\n","Fill out the function `ticker_to_name()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `StringType()`. Ensure that your function returns this. You must also deal with any potential `null` cases."]},{"cell_type":"code","metadata":{"id":"P4cJWZsr8iNC","colab_type":"code","outputId":"b03c74fd-3055-4967-81e4-6c98a1a26cf0","executionInfo":{"status":"ok","timestamp":1576004691234,"user_tz":300,"elapsed":878,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# TODO: Fill out [ticker_to_name()] and register it as a udf.\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# Dictionary linking stock ticker's to their name\n","ticker_to_name_dict = {'NOK': 'Nokia',\n","                       'UN': 'Unilever',\n","                       'BP': 'BP',\n","                       'JNJ': 'Johnson & Johnson',\n","                       'TCS': 'Tata Consultancy Services',\n","                       'SLB': 'Schlumberger',\n","                       'NVS': 'Novartis',\n","                       'CNY': 'Huawei',\n","                       'PFE': 'Pfizer',\n","                       'ACN': 'Accenture',\n","                       'DELL': 'Dell',\n","                       'MS': 'Morgan Stanley',\n","                       'ORCL': 'Oracle',\n","                       'BAC': 'Bank of America',\n","                       'PG': 'Procter & Gamble',\n","                       'CGEMY': 'Capgemini',\n","                       'GS': 'Goldman Sachs',\n","                       'C': 'Citi',\n","                       'IBM': 'IBM',\n","                       'CS': 'Credit Suisse',\n","                       'MDLZ': 'Kraft Foods',\n","                       'WIT': 'Wipro Technologies',\n","                       'CSCO': 'Cisco Systems',\n","                       'PWC': 'PwC',\n","                       'GOOGL': 'Google',\n","                       'CTSH': 'Cognizant Technology Solutions',\n","                       'HSBC': 'HSBC',\n","                       'DB': 'Deutsche Bank',\n","                       'MSFT': 'Microsoft',\n","                       'HPE': 'Hewlett-Packard',\n","                       'ERIC': 'Ericsson',\n","                       'BCS': 'Barclays Capital',\n","                       'GSK': 'GlaxoSmithKline'}\n","\n","# Fill out ticker_to_name()\n","\n","# In UDFs we have to cover all possible output cases, or else the function will\n","# crash. Specifically, this means we need to handle the case when \"ticker\" is\n","# not in \"ticker_to_name_dict\". We use a try and except statement to return null\n","# for this case.\n","def ticker_to_name(ticker):\n","  try:\n","    return ticker_to_name_dict[ticker]\n","  except:\n","    return None\n","\n","### END SOLUTION\n","# Register udf as a SQL function. DO NOT EDIT\n","spark.udf.register(\"TICKER_TO_NAME\", ticker_to_name, StringType())\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.ticker_to_name>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"u9YOYO9L-_GS","colab_type":"text"},"source":["#### Step 3.4.2: The Fastidious Filters\n","\n","With our new `TICKER_TO_NAME()` function we will begin to wrangle `raw_stocks_sdf`.\n","\n","Create an sdf called `filter_1_stocks_sdf` as follows. Convert all the ticker names in `raw_stocks_sdf` to the company names and save it as `org`. Next, convert the `date` field to a datetime type. As explained before this will help order and group the rows in future steps. Then, convert the type of the values in `closing_price` to `float`. This will take care of the `dtypes` issue we saw in Step 3.3.\n","\n","Drop any company names that do not appear in `ticker_to_name_dict`. Keep any date between January 1st 2001 and December 4th 2012 inclusive, in the format shown below (note this is a datetime object not a string):\n","\n","```\n","+----+------------+--------------+\n","|org |date        |closing_price |\n","+----+------------+--------------+\n","|IBM |2000-01-03  |...           |\n","|... |...         |...           |\n","+----+------------+--------------+\n","```\n","_Hint_: You will use a similar function to filter the dates as in Step 1.4. In Spark SQL the format for the `date` field in `raw_stocks_sdf` is `\"yyyy-MM-dd\"`."]},{"cell_type":"code","metadata":{"id":"RuiitnWlBYJ7","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_1_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# Format the \"ticker\" column using our UDF, TICKER_TO_NAME. Use TO_DATE() to\n","# convert the string date column to datetime object and filter on this in the\n","# same way as Step 1.4\n","query = '''SELECT TICKER_TO_NAME(ticker) AS org,\n","                  TO_DATE(date, \"yyyy-MM-dd\") AS date,\n","                  CAST(closing_price AS FLOAT)\n","           FROM raw_stocks\n","           WHERE TO_DATE(date, \"yyyy-MM-dd\") >= \"2001-01-01\"\n","                 AND TO_DATE(date, \"yyyy-MM-dd\") <= \"2012-12-04\"'''\n","\n","# Define and save filter_1_stocks_sdf\n","filter_1_stocks_sdf = spark.sql(query).dropna()\n","filter_1_stocks_sdf.createOrReplaceTempView(\"filter_1_stocks\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ne5NaT-6CLns","colab_type":"text"},"source":["#### Step 3.4.3: The Magnanimous Months\n","\n","The data in `filter_1_stocks_sdf` gives closing prices on a daily basis. Since we are interested in monthly trends, we will only keep the closing price on the **last trading day of each month**.\n","\n","Create an sdf `filter_2_stocks_sdf` that contains only the closing prices for the last trading day of each month. Note that a trading day is not simply the last day of each month, as this could be on a weekend when the market is closed . The format of the sdf is shown below:\n","\n","```\n","+----+------------+--------------+\n","|org |date        |closing_price |\n","+----+------------+--------------+\n","|IBM |2000-01-31  |...           |\n","|... |...         |...           |\n","+----+------------+--------------+\n","```\n","\n","  _Hint_: It may be helpful to create an intermediate dataframe that will help you filter out the specific dates you desire."]},{"cell_type":"code","metadata":{"id":"AIx5LUuDD4q_","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_2_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# Create sdf that has for each company, the closing day for each month. We need\n","# to preform a GROUP BY on three features, org, YEAR(date), and MONTH(date).\n","# This will give us aggregations of the closing stock price for every day of a\n","# specified month and a specified year. Since these are all datetime objects,\n","# taking MAX() will give us the highest, i.e. last, one.\n","query = '''SELECT org,\n","                  MAX(date) AS close_date\n","           FROM filter_1_stocks\n","           GROUP BY org, YEAR(date), MONTH(date)'''\n","\n","# Save as a temporary sdf, desired_months\n","spark.sql(query).createOrReplaceTempView(\"desired_months\")\n","\n","# Merge desired_months with filter_1_stocks. This will allow us to keep the\n","# closing prices for only those dates that were the closing date for a given\n","# month.\n","query = '''SELECT f.org AS org,\n","                  m.close_date AS date,\n","                  f.closing_price AS closing_price\n","           FROM filter_1_stocks AS f\n","           JOIN desired_months AS m\n","           ON f.org = m.org AND f.date = m.close_date'''\n","\n","# Define and save filter_2_stocks_sdf\n","filter_2_stocks_sdf = spark.sql(query)\n","filter_2_stocks_sdf.createOrReplaceTempView(\"filter_2_stocks\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AG4bACKKEQNl","colab_type":"text"},"source":["#### Step 3.4.4: The Rambunctious Reshape\n","\n","Now, we will begin to shape our dataframe into the format of the final training sdf.\n","\n","Create an sdf `filter_3_stocks_sdf` that has for a single company and a single year, the closing stock price for the last trading day of each month in that year. This is similar to the table you created in Step 3.1. In this case since we cannot make a proxy for the closing price if the data is not avaliable, drop any rows containing any `null` values, in any column. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+\n","|org |year |jan_stock |   ...   |dec_stock |\n","+----+-----+----------+---------+----------+\n","|IBM |2008 |...       |   ...   |...       |\n","|IBM |2009 |...       |   ...   |...       |\n","|... |...  |...       |   ...   |...       |\n","+----+-----+----------+---------+----------+\n","```\n"]},{"cell_type":"code","metadata":{"id":"AucLEgvwIr_0","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_3_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# We will do the same operation we did in Step 3.1. In this case, however, as\n","# the question specifies, any missing entry in a given month are set to null.\n","query = '''SELECT org, \n","                  YEAR(date) AS year,\n","                  MAX((CASE WHEN (MONTH(date) == 1) \n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS jan_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 2)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS feb_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 3)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS mar_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 4)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS apr_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 5)\n","                                  THEN closing_price \n","                                  ELSE NULL END)) AS may_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 6)\n","                                  THEN closing_price\n","                                   ELSE NULL END)) AS jun_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 7)\n","                                  THEN closing_price \n","                                  ELSE NULL END)) AS jul_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 8)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS aug_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 9)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS sep_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 10)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS oct_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 11)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS nov_stock,\n","                  MAX((CASE WHEN (MONTH(date) == 12)\n","                                  THEN closing_price\n","                                  ELSE NULL END)) AS dec_stock\n","           FROM filter_2_stocks\n","           GROUP BY org, YEAR(date)\n","           ORDER BY org, year ASC'''\n","\n","# Define and save filter_3_stocks_sdf\n","filter_3_stocks_sdf = spark.sql(query).dropna()\n","filter_3_stocks_sdf.createOrReplaceTempView(\"filter_3_stocks\")\n","\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"82OQKp-nIulq","colab_type":"text"},"source":["#### Step 3.4.5: The Decisive Direction\n","\n","The final element in our training set is the binary output for each case, i.e. the `y` label. \n","\n","Create an sdf `stocks_train_sdf` from `filter_3_stocks_sdf` with an additional column `direction`. This should be the direction of percentage change in the closing stock price, i.e. `1` for positive or `-1` for negative, in the first quarter of a given year. The quarter of a year begins in January and ends in April, inclusive. We want to know the percent change between these two months. Reference Step 2.2 for the percent change formula. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+-------------+\n","|org |year |jan_stock |   ...   |dec_stock |direction    |\n","+----+-----+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |1.0          |\n","|IBM |2009 |...       |   ...   |...       |-1.0         |\n","|... |...  |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+-------------+\n","```"]},{"cell_type":"code","metadata":{"id":"yEFJIfyZKf7B","colab_type":"code","colab":{}},"source":["# TODO: Create [stocks_train_sdf]\n","\n"," ## YOUR SOLUTION HERE\n"," ### BEGIN SOLUTION\n","# SIGN() will return -1 if the input is negative, 0 if the input is zero, and 1\n","# if the input is positive.\n","\n","# Keep all rows in filter_3_stocks and add another based on the sign of the\n","# percentage change in stock\n","query = '''SELECT *,\n","                  SIGN(apr_stock-jan_stock) AS direction\n","           FROM filter_3_stocks'''\n","\n","# Define and save stocks_train_sdf\n","stocks_train_sdf = spark.sql(query)\n","stocks_train_sdf.createOrReplaceTempView(\"stocks_train\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fd2nviNpM2dF","colab_type":"text"},"source":["### Step 3.5: The Capricious Combination\n","\n","Now that we have individually created the two halfs of our training data we will merge them together to create the final training sdf we showed in the beginning of Step 3.\n","\n","Create an sdf called `training_sdf` in the format of the one shown at the beginning of Step 3. Note that in our definition for the `stock_result` column, the `stock_result` value for a particular year corresponds to the direction of the stock percentage change in the **following** year. For example, the stock_result in the `2008` row for `IBM` will contain the direction of IBM's stock in the first quarter of 2009. The format of the sdf is shown below:\n","```\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |-1.0         |\n","|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |1.0          |\n","|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","```"]},{"cell_type":"code","metadata":{"id":"8ZIb6QkcO5RB","colab_type":"code","colab":{}},"source":["# TODO: Create [training_sdf]\n","\n","## YOUR SOLUTION HERE\n","### BEGIN SOLUTION\n","# Our merge will consist of two joins. The first will use filter_3_stocks to\n","# join the monthly hiring rates and closing prices. The next join will be with\n","# stock_train and to find stock_result. This join will be done such that the\n","# correct years are matched between hire_train and stocks_train\n","query = '''SELECT t1.*,\n","                  s.direction AS stock_result\n","           FROM (SELECT h.org,\n","                        h.year,\n","                        h.jan_hired,\n","                        h.feb_hired,\n","                        h.mar_hired,\n","                        h.apr_hired,\n","                        h.may_hired,\n","                        h.jun_hired,\n","                        h.jul_hired,\n","                        h.aug_hired,\n","                        h.sep_hired,\n","                        h.oct_hired,\n","                        h.nov_hired,\n","                        h.dec_hired,\n","                        f.jan_stock,\n","                        f.feb_stock,\n","                        f.mar_stock,\n","                        f.apr_stock,\n","                        f.may_stock,\n","                        f.jun_stock,\n","                        f.jul_stock,\n","                        f.aug_stock,\n","                        f.sep_stock,\n","                        f.oct_stock,\n","                        f.nov_stock,\n","                        f.dec_stock\n","                 FROM hire_train AS h\n","                 JOIN filter_3_stocks AS f\n","                 ON h.org = f.org AND h.year = f.year) AS t1\n","           JOIN stocks_train AS s\n","           ON t1.org = s.org AND s.year = (t1.year + 1)'''\n","\n","# Define and save training_sdf\n","training_sdf = spark.sql(query)\n","training_sdf.createOrReplaceTempView(\"training\")\n","### END SOLUTION"],"execution_count":0,"outputs":[]}]}