{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lecture_Notebook_Local_New_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp",
        "colab_type": "text"
      },
      "source": [
        "# Lecture Notebook: Big Data and Graph Data\n",
        "\n",
        "Apache Spark is a big data engine that runs on compute clusters, including on the cloud.  Since not everyone will have access to a compute cluster, this version of the notebook is set up to run Spark locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPw_ePbge5xR",
        "colab_type": "code",
        "outputId": "cb07013c-ca4d-4217-eb02-fb8aac01ed28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt install libkrb5-dev\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libkrb5-dev is already the newest version (1.16-2ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Requirement already satisfied: sparkmagic in /usr/local/lib/python3.6/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (1.17.5)\n",
            "Requirement already satisfied: hdijupyterutils>=0.6 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.15.0)\n",
            "Requirement already satisfied: autovizwidget>=0.6 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.15.0)\n",
            "Requirement already satisfied: requests-kerberos>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.12.0)\n",
            "Requirement already satisfied: notebook>=4.2 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets>5.0.0 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (7.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (5.5.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.6.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.25.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (2.21.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.5.3)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (1.3.7)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.0.1)\n",
            "Requirement already satisfied: jupyter>=1 in /usr/local/lib/python3.6/dist-packages (from hdijupyterutils>=0.6->sparkmagic) (1.0.0)\n",
            "Requirement already satisfied: plotly>=3 in /usr/local/lib/python3.6/dist-packages (from autovizwidget>=0.6->sparkmagic) (4.4.1)\n",
            "Requirement already satisfied: cryptography>=1.3; python_version != \"3.3\" in /usr/local/lib/python3.6/dist-packages (from requests-kerberos>=0.8.0->sparkmagic) (2.8)\n",
            "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from requests-kerberos>=0.8.0->sparkmagic) (1.2.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (2.11.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (4.3.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.3.4)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (0.8.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (4.6.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.5.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (45.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (4.4.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->sparkmagic) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->sparkmagic) (2.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (1.24.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (4.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (1.12.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (1.3.3)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=1.3; python_version != \"3.3\"->requests-kerberos>=0.8.0->sparkmagic) (1.14.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.2->sparkmagic) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook>=4.2->sparkmagic) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook>=4.2->sparkmagic) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.2->sparkmagic) (0.1.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=1.3; python_version != \"3.3\"->requests-kerberos>=0.8.0->sparkmagic) (2.19)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.2->sparkmagic) (0.5.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrFAyi5re65W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3cf8d546-dd49-4169-85b1-d00034b4e28d"
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sparkmagic.magics extension is already loaded. To reload it, use:\n",
            "  %reload_ext sparkmagic.magics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr8AfhqNG1_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    import os\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    import pyspark\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.types import *\n",
        "    import pyspark.sql.functions as F\n",
        "    from pyspark.sql import SQLContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmtMSb0gHh7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    try:\n",
        "       if(spark == None):\n",
        "            spark = SparkSession.builder.appName('Graphs').getOrCreate()\n",
        "            sqlContext=SQLContext(spark)\n",
        "    except NameError:\n",
        "        spark = SparkSession.builder.appName('Graphs').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u6rwG-wQIX2",
        "colab_type": "text"
      },
      "source": [
        "## Example of Loading Sharded Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O02TOXqQLgt",
        "colab_type": "text"
      },
      "source": [
        "First let's load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDHoasDnQVLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5oUyj7RmHcZ",
        "colab_type": "code",
        "outputId": "3b6445be-a03d-4f40-a3e0-bd8079afda5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If use Colab and want to mount Google Drive to it. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmXoH1gmtD2d",
        "colab_type": "code",
        "outputId": "0e04b413-0573-4eb1-b6c6-369acf9b9c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get 10K synthetic linked records from url X. \n",
        "#linked_in = requests.get('X')\n",
        "# my_list = [json.loads(line) for line in linked_in.iter_lines()]\n",
        "\n",
        "# Or to use that file locally. \n",
        "linked_in = open('/content/drive/My Drive/Colab Notebooks/test_data_10000.json')\n",
        "my_list = [json.loads(line) for line in linked_in]\n",
        "\n",
        "len(my_list)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-Or-F-Qc5C",
        "colab_type": "text"
      },
      "source": [
        "## Load the list into Spark\n",
        "\n",
        "Spark needs to know the structure of the data in its dataframes, i.e., their schemas.  Since our JSON structure for LinkedIn is complex, we need to define the schema.\n",
        "\n",
        "There are some basic types:\n",
        "  * The table is a `StructType` with a list of fields (each row)\n",
        "  * Most fields, in our case, are `StringType`.\n",
        "  * We also have nested dictionary for the name, which is a `MapType` from `StringType` keys to `StringType` values.\n",
        "  * `skills` is an `ArrayType` since it's a list, and it contains `StringType`s.\n",
        "  * `also_view` is an array of structs.\n",
        "\n",
        "See Pyspark documentation on `StructType` and examples such as https://www.programcreek.com/python/example/104715/pyspark.sql.types.StructType."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN3NJaRuP8Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spark requires that we define a schema for the LinkedIn data...\n",
        "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, MapType\n",
        "schema = StructType([\n",
        "        StructField(\"_id\", StringType(), True),\n",
        "        StructField(\"name\", MapType(StringType(), StringType()), True),\n",
        "        StructField(\"locality\", StringType(), True),\n",
        "        StructField(\"skills\", ArrayType(StringType()), True),\n",
        "        StructField(\"industry\", StringType(), True),\n",
        "        StructField(\"summary\", StringType(), True),\n",
        "        StructField(\"url\", StringType(), True),\n",
        "        StructField(\"also_view\", ArrayType(\\\n",
        "                    StructType([\\\n",
        "                      StructField(\"url\", StringType(), True),\\\n",
        "                      StructField(\"id\", StringType(), True)])\\\n",
        "                    ), True)\\\n",
        "         ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmF8gC6nP3VW",
        "colab_type": "code",
        "outputId": "21dca6ad-4143-494e-e024-ed8d04e456aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Load the remote data as a list of dictionaries\n",
        "linked_df = sqlContext.createDataFrame(my_list, schema).\\\n",
        "      repartition('_id')\n",
        "\n",
        "linked_df.show(5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                 _id|                name|           locality|              skills|            industry|             summary|                 url|           also_view|\n",
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|nvgdapwhxjjzsvycn...|[given_name -> M ...|    Uppsala, Sweden|                  []|    Ämter & Behörden|Ashley Dunning is...|jzvdtmrfybdqgeghl...|[[http://uk.linke...|\n",
            "|uqerqpzcdggyrnqun...|[given_name -> Ab...|          Singapore|[{value=Motion Co...|Information Techn...|Regional Sales Ma...|cvwkibsajmjkesjvy...|[[http://in.linke...|\n",
            "|wyzoingfdqnkqrvuq...|[given_name -> Ad...|Houston, Texas Area|[{value=Managemen...|Pengambilan Kakit...|I am a training a...|babrthcyozvewpzkz...|[[http://in.linke...|\n",
            "|yitfbojzyhfmhdwdx...|[given_name -> An...| Paris Area, France|                  []|Technologies et s...|Social Network & ...|nucvqsmlflqnrkwzl...|[[http://www.link...|\n",
            "|gliizzsriqmgyfqop...|[given_name -> St...|            Türkiye|[{value=Brand Man...|Health, Wellness ...|Results-driven pe...|lllncsplqsprcsaia...|[[http://au.linke...|\n",
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abqKt2z0ni4u",
        "colab_type": "code",
        "outputId": "444a1561-caf1-4bfe-cd2b-a8464036c6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "linked_df.select('name').show( truncate = False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|name                                                              |\n",
            "+------------------------------------------------------------------+\n",
            "|[given_name -> M Burak, family_name -> Bhandari]                  |\n",
            "|[given_name -> Abdelhay, family_name -> da Silva dos Santos]      |\n",
            "|[given_name -> Adnan Maqbool - SAP, family_name -> Masloski]      |\n",
            "|[given_name -> Anbazhagan (Anbu), family_name -> Aguinaga Azpiazu]|\n",
            "|[given_name -> Steve, family_name -> Geisler]                     |\n",
            "|[given_name -> Aysel, family_name -> Singer]                      |\n",
            "|[given_name -> Abraham, family_name -> Asgeirsson]                |\n",
            "|[given_name -> Agatha, family_name -> Choung]                     |\n",
            "|[given_name -> Sean, family_name -> Ekelund]                      |\n",
            "|[given_name -> Aarushi, family_name -> Åkesson]                   |\n",
            "|[given_name -> Andrew A., family_name -> Baahmed]                 |\n",
            "|[given_name -> Agner, family_name -> Battestini]                  |\n",
            "|[given_name -> Agibson, family_name -> Somers]                    |\n",
            "|[given_name -> Ashok, family_name -> Kenney]                      |\n",
            "|[given_name -> Franz, family_name -> Khanna (MCTS SharePoint)]    |\n",
            "|[given_name -> Adrien, family_name -> Carluccio]                  |\n",
            "|[given_name -> Charu, family_name -> Elvingsson {-_-}]            |\n",
            "|[given_name -> Abin, family_name -> Berry]                        |\n",
            "|[given_name -> Adama, family_name -> Long]                        |\n",
            "|[given_name -> R Akila, family_name -> Erratt]                    |\n",
            "+------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8TnR04oJVoJ",
        "colab_type": "code",
        "outputId": "837ae21e-7e52-49d1-9be5-2d52f956dfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "linked_df.rdd.getNumPartitions()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUo_4LfOiki",
        "colab_type": "code",
        "outputId": "b099e792-956e-47c4-95d1-dd7fa26f61fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "linked_df.filter(linked_df.locality == 'United States')[['_id', 'name', 'locality']].show(5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------------+\n",
            "|                 _id|                name|     locality|\n",
            "+--------------------+--------------------+-------------+\n",
            "|dslvikjojvqncpxag...|[given_name -> Mo...|United States|\n",
            "|eavnmharasuudkshz...|[given_name -> Je...|United States|\n",
            "|zmjjbbtwfssfsgxnq...|[given_name -> Po...|United States|\n",
            "|miagrwbtyjfnrhdvg...|[given_name -> Ag...|United States|\n",
            "|pfxsthrqjqfzkvlkf...|[given_name -> Ad...|United States|\n",
            "+--------------------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj1E3-AJ1G_y",
        "colab_type": "code",
        "outputId": "0a757f6c-655f-4870-db18-709123303211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "linked_df.select(\"_id\", \"locality\").show(5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+\n",
            "|                 _id|           locality|\n",
            "+--------------------+-------------------+\n",
            "|nvgdapwhxjjzsvycn...|    Uppsala, Sweden|\n",
            "|uqerqpzcdggyrnqun...|          Singapore|\n",
            "|wyzoingfdqnkqrvuq...|Houston, Texas Area|\n",
            "|yitfbojzyhfmhdwdx...| Paris Area, France|\n",
            "|gliizzsriqmgyfqop...|            Türkiye|\n",
            "+--------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e68gcPiaL5Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Clean out the list from memory\n",
        "my_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD9SE_v-1v0C",
        "colab_type": "code",
        "outputId": "9a709641-feef-4b3c-e8b7-921980d7f84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "linked_df.createOrReplaceTempView('linked_in')\n",
        "sqlContext.sql('select * from linked_in').show(5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                 _id|                name|           locality|              skills|            industry|             summary|                 url|           also_view|\n",
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|nvgdapwhxjjzsvycn...|[given_name -> M ...|    Uppsala, Sweden|                  []|    Ämter & Behörden|Ashley Dunning is...|jzvdtmrfybdqgeghl...|[[http://uk.linke...|\n",
            "|uqerqpzcdggyrnqun...|[given_name -> Ab...|          Singapore|[{value=Motion Co...|Information Techn...|Regional Sales Ma...|cvwkibsajmjkesjvy...|[[http://in.linke...|\n",
            "|wyzoingfdqnkqrvuq...|[given_name -> Ad...|Houston, Texas Area|[{value=Managemen...|Pengambilan Kakit...|I am a training a...|babrthcyozvewpzkz...|[[http://in.linke...|\n",
            "|yitfbojzyhfmhdwdx...|[given_name -> An...| Paris Area, France|                  []|Technologies et s...|Social Network & ...|nucvqsmlflqnrkwzl...|[[http://www.link...|\n",
            "|gliizzsriqmgyfqop...|[given_name -> St...|            Türkiye|[{value=Brand Man...|Health, Wellness ...|Results-driven pe...|lllncsplqsprcsaia...|[[http://au.linke...|\n",
            "+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yur38PPmVGt-",
        "colab_type": "code",
        "outputId": "1618f1fe-c6e8-448b-eae9-04edd1c83977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sqlContext.sql('select _id, name.given_name, name.family_name from linked_in').show(10)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-------------------+\n",
            "|                 _id|         given_name|        family_name|\n",
            "+--------------------+-------------------+-------------------+\n",
            "|nvgdapwhxjjzsvycn...|            M Burak|           Bhandari|\n",
            "|uqerqpzcdggyrnqun...|           Abdelhay|da Silva dos Santos|\n",
            "|wyzoingfdqnkqrvuq...|Adnan Maqbool - SAP|           Masloski|\n",
            "|yitfbojzyhfmhdwdx...|  Anbazhagan (Anbu)|   Aguinaga Azpiazu|\n",
            "|gliizzsriqmgyfqop...|              Steve|            Geisler|\n",
            "|nilkycgqibqzhcpws...|              Aysel|             Singer|\n",
            "|cbeqtauvkrpwjdxno...|            Abraham|         Asgeirsson|\n",
            "|aamexaalrxcxvcvmt...|             Agatha|             Choung|\n",
            "|ojpxbwcehftbsicle...|               Sean|            Ekelund|\n",
            "|bpfnuxzybuusoiubr...|            Aarushi|            Åkesson|\n",
            "+--------------------+-------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcvsuNw9Tr6k",
        "colab_type": "code",
        "outputId": "43c24110-07bf-42a8-a6e3-7afffdeb1b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "acro = udf(lambda x: ''.join([n[0] for n in x.split()]), StringType())\n",
        "\n",
        "linked_df.select(\"_id\", acro(\"locality\").alias(\"acronym\")).show(5)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+\n",
            "|                 _id|acronym|\n",
            "+--------------------+-------+\n",
            "|nvgdapwhxjjzsvycn...|     US|\n",
            "|uqerqpzcdggyrnqun...|      S|\n",
            "|wyzoingfdqnkqrvuq...|    HTA|\n",
            "|yitfbojzyhfmhdwdx...|    PAF|\n",
            "|gliizzsriqmgyfqop...|      T|\n",
            "+--------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMW4qFCQf4W",
        "colab_type": "code",
        "outputId": "d65347ba-15ce-4eba-b9ff-1a531b3c4def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Which industries are most popular?\n",
        "sqlContext.sql('select count(_id), industry '+\\\n",
        "               'from linked_in '+\\\n",
        "               'group by industry '+\\\n",
        "               'order by count(_id) desc').\\\n",
        "    show(5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|count(_id)|            industry|\n",
            "+----------+--------------------+\n",
            "|       323|Information Techn...|\n",
            "|       212|   Computer Software|\n",
            "|       112|            Internet|\n",
            "|       102|Marketing and Adv...|\n",
            "|        72|  Financial Services|\n",
            "+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrsSWsmI7pG",
        "colab_type": "text"
      },
      "source": [
        "## Graphs\n",
        "\n",
        "For the next set of examples, we will look at graph-structured data.  It turns out our LinkedIn dataset has a list of nodes (by int ID, but associated with the user ID we used in the linked_in table) and a list of edges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLkmsCn02FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace the URL with the correct address of the edges.zip file\n",
        "url = 'https://xxx/opends4all/linkedin.edges.zip'\n",
        "filehandle, _ = urllib.request.urlretrieve(url)\n",
        "\n",
        "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
        "fname = zip_file_object.open('linkedin.edges')\n",
        "\n",
        "edges = []\n",
        "MAX = 2000000\n",
        "\n",
        "for link in fname:\n",
        "  edge = link.decode('utf-8').split(' ')\n",
        "  edges.append([int(edge[0]), int(edge[1])])\n",
        "  if len(edges) >= MAX:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10raoCWVJQXk",
        "colab_type": "code",
        "outputId": "55ad9f4c-83d2-4a28-eb09-4d38fb160196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "schema = StructType([\n",
        "        StructField(\"from\", IntegerType(), True),\n",
        "        StructField(\"to\", IntegerType(), True)\n",
        "         ])\n",
        "# Load the remote data as a list of dictionaries\n",
        "edges_df = sqlContext.createDataFrame(edges, schema)\n",
        "\n",
        "edges_df.show(5)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+\n",
            "|   from|     to|\n",
            "+-------+-------+\n",
            "|2282701|3912501|\n",
            "|2282701|5182389|\n",
            "|2282701|3822131|\n",
            "|2282701|8034158|\n",
            "|2282701|1946731|\n",
            "+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4WTTHohshi",
        "colab_type": "code",
        "outputId": "d72cc011-5940-4d90-e532-82131bbea307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "edges_df.createOrReplaceTempView('edges')\n",
        "sqlContext.sql('select from as id, count(to) as degree from edges group by from').show(5)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+\n",
            "|     id|degree|\n",
            "+-------+------+\n",
            "|7221550|   148|\n",
            "|6476312|   130|\n",
            "|4200201|   129|\n",
            "|6494598|   106|\n",
            "|1223588|    89|\n",
            "+-------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbXkhbzOk1ZK",
        "colab_type": "text"
      },
      "source": [
        "## Traversing the Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJ_BjL1kiQi",
        "colab_type": "code",
        "outputId": "1ca61e7a-bb61-4bcc-b86e-b9df2715ec67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Start with a subset of nodes\n",
        "start_nodes_df = edges_df[['from']].filter(edges_df['from'] < 100000).\\\n",
        "  select(col('from').alias('id')).drop_duplicates()\n",
        "\n",
        "start_nodes_df.show(5)\n",
        "\n",
        "# The neighbors require us to join\n",
        "# and we'll use Spark DataFrames syntax here\n",
        "neighbor_nodes_df = start_nodes_df.\\\n",
        "  join(edges_df, start_nodes_df.id == edges_df['from']).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_nodes_df.show(5)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|   id|\n",
            "+-----+\n",
            "|71510|\n",
            "|74058|\n",
            "|76756|\n",
            "|  148|\n",
            "|50353|\n",
            "+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+\n",
            "|     id|\n",
            "+-------+\n",
            "|9777199|\n",
            "|1743876|\n",
            "|2207408|\n",
            "|7836663|\n",
            "|9431178|\n",
            "+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpoPxuPQnmNQ",
        "colab_type": "code",
        "outputId": "70f1fb38-6dc8-48c5-bf79-26846c68074c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "edges_df[['from']].orderBy('from').drop_duplicates().show()\n",
        "\n",
        "edges_df.filter(edges_df['from'] == 698).show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|from|\n",
            "+----+\n",
            "| 123|\n",
            "| 148|\n",
            "| 322|\n",
            "| 698|\n",
            "|1077|\n",
            "|1144|\n",
            "|1162|\n",
            "|1302|\n",
            "|1393|\n",
            "|1504|\n",
            "|1525|\n",
            "|1638|\n",
            "|1678|\n",
            "|1808|\n",
            "|1939|\n",
            "|2163|\n",
            "|2514|\n",
            "|2582|\n",
            "|2714|\n",
            "|2952|\n",
            "+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+-------+\n",
            "|from|     to|\n",
            "+----+-------+\n",
            "| 698|4067826|\n",
            "| 698| 565155|\n",
            "| 698|1009367|\n",
            "| 698|7264684|\n",
            "| 698|8609691|\n",
            "| 698|8797222|\n",
            "| 698|2078369|\n",
            "| 698|9390344|\n",
            "| 698|2343229|\n",
            "| 698|4874249|\n",
            "| 698|2545215|\n",
            "| 698|3127567|\n",
            "| 698|4760779|\n",
            "| 698|4460232|\n",
            "| 698|4674327|\n",
            "| 698|7848239|\n",
            "| 698|2324983|\n",
            "| 698|9215385|\n",
            "| 698|5698971|\n",
            "| 698|5996191|\n",
            "+----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pnH4lKqNiV",
        "colab_type": "code",
        "outputId": "dfde791e-90a2-41ce-c658-173442e5c67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "neighbor_neighbor_nodes_df = neighbor_nodes_df.\\\n",
        "  join(edges_df, neighbor_nodes_df.id == edges_df['from']).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_neighbor_nodes_df.show(5)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|     id|\n",
            "+-------+\n",
            "|6035498|\n",
            "|6166961|\n",
            "|2448237|\n",
            "|1905292|\n",
            "|1747364|\n",
            "+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvsJ2_LNsTbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate(df, depth):\n",
        "  df.createOrReplaceTempView('iter')\n",
        "\n",
        "  # Base case: direct connection\n",
        "  result = sqlContext.sql('select from, to, 1 as depth from iter')\n",
        "\n",
        "  for i in range(1, depth):\n",
        "    result.createOrReplaceTempView('result')\n",
        "    result = sqlContext.sql('select r1.from as from, r2.to as to, r1.depth+1 as depth  '\\\n",
        "                            'from result r1 join iter r2 '\\\n",
        "                            'on r1.to=r2.from')\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiE4BBMuwqv",
        "colab_type": "code",
        "outputId": "54a04c19-270e-48aa-91f3-5bb995530abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 1).orderBy('from','to').show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+-----+\n",
            "|from|     to|depth|\n",
            "+----+-------+-----+\n",
            "| 123|  91543|    1|\n",
            "| 123| 766201|    1|\n",
            "| 123| 805244|    1|\n",
            "| 123|1115889|    1|\n",
            "| 123|1460837|    1|\n",
            "| 123|1837525|    1|\n",
            "| 123|2397963|    1|\n",
            "| 123|3117349|    1|\n",
            "| 123|3499006|    1|\n",
            "| 123|4090527|    1|\n",
            "| 123|4937958|    1|\n",
            "| 123|5057050|    1|\n",
            "| 123|6277751|    1|\n",
            "| 123|7789741|    1|\n",
            "| 123|7956667|    1|\n",
            "| 123|8022868|    1|\n",
            "| 123|8212249|    1|\n",
            "| 123|8688764|    1|\n",
            "| 123|9166545|    1|\n",
            "| 123|9283427|    1|\n",
            "+----+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63RGiC1euw-y",
        "colab_type": "code",
        "outputId": "75043ffa-d8bd-4d22-cdd2-78a34b501963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 2).orderBy('from','to').show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+-----+\n",
            "|from|     to|depth|\n",
            "+----+-------+-----+\n",
            "|1678| 868412|    2|\n",
            "|1678|1855400|    2|\n",
            "|1678|2762625|    2|\n",
            "|1678|3558053|    2|\n",
            "|1678|4929260|    2|\n",
            "|1678|5112663|    2|\n",
            "|1678|5459172|    2|\n",
            "|1678|6132561|    2|\n",
            "|1678|6314741|    2|\n",
            "|1678|6444050|    2|\n",
            "|1678|7038299|    2|\n",
            "|1678|7394947|    2|\n",
            "|1678|7802268|    2|\n",
            "|1678|8364779|    2|\n",
            "|1678|8504601|    2|\n",
            "|1678|9401691|    2|\n",
            "|1678|9416271|    2|\n",
            "|1678|9993870|    2|\n",
            "|2582| 288213|    2|\n",
            "|2582| 504980|    2|\n",
            "+----+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wG7vMrXuxFr",
        "colab_type": "code",
        "outputId": "8d25b3f0-7962-4525-f8c7-d08daa8ee459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "iterate(edges_df.filter(edges_df['from'] < 1000000), 3).orderBy('from','to').show()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-------+-----+\n",
            "| from|     to|depth|\n",
            "+-----+-------+-----+\n",
            "|15305| 859468|    3|\n",
            "|15305| 919212|    3|\n",
            "|15305|1032892|    3|\n",
            "|15305|1079033|    3|\n",
            "|15305|1346621|    3|\n",
            "|15305|2324259|    3|\n",
            "|15305|2451906|    3|\n",
            "|15305|3249045|    3|\n",
            "|15305|3696999|    3|\n",
            "|15305|5032700|    3|\n",
            "|15305|5197855|    3|\n",
            "|15305|5447389|    3|\n",
            "|15305|6836284|    3|\n",
            "|15305|7912140|    3|\n",
            "|15305|8331813|    3|\n",
            "|15305|9103098|    3|\n",
            "|15305|9826214|    3|\n",
            "|16247| 731360|    3|\n",
            "|16247| 733043|    3|\n",
            "|16247|1893329|    3|\n",
            "+-----+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N1eHyunMccX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear list of edges from Python memory\n",
        "# to free up space\n",
        "edges = []\n",
        "\n",
        "\n",
        "### Now let's get the list of node IDs\n",
        "url='https://xxx/opends4all/linkedin.nodes.zip'\n",
        "nodehandle, _ = urllib.request.urlretrieve(url)\n",
        "\n",
        "zip_file_object = zipfile.ZipFile(nodehandle, 'r')\n",
        "fname = zip_file_object.open('linkedin.nodes')\n",
        "\n",
        "nodes = []\n",
        "MAX = 100000\n",
        "\n",
        "for node in fname:\n",
        "  node_tuple = node.decode('utf-8').split()\n",
        "  nodes.append([int(node_tuple[0]), str(node_tuple[1])])\n",
        "  if len(nodes) >= MAX:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCba0mxkikw",
        "colab_type": "text"
      },
      "source": [
        "## Joins in Spark, Beyond Graph Traversals\n",
        "\n",
        "What if we want to connect our edges to the people from our previous crawl?  Sadly the edges use int node IDs that don't correspond to the people dataframe.  But in fact the node data includes this information, so let's load and exploit that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wg-2C56pP_s",
        "colab_type": "text"
      },
      "source": [
        "Let's load the information about nodes, and their correspondence to the user ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3iZL-pWX8xd",
        "colab_type": "code",
        "outputId": "80cf104c-2a58-4228-f34c-f24beb23e790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "schema = StructType([\n",
        "        StructField(\"nid\", IntegerType(), True),\n",
        "        StructField(\"user\", StringType(), True)\n",
        "         ])\n",
        "# Load the remote data as a list of dictionaries\n",
        "nodes_df = sqlContext.createDataFrame(nodes, schema)\n",
        "\n",
        "nodes_df.show(5)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------+\n",
            "|    nid|         user|\n",
            "+-------+-------------+\n",
            "|2282701|newin_2282701|\n",
            "|9582258|newin_9582258|\n",
            "| 853296| newin_853296|\n",
            "| 869746| newin_869746|\n",
            "|6899568|newin_6899568|\n",
            "+-------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVwMCIspVPc",
        "colab_type": "text"
      },
      "source": [
        "## Finding Friends, by ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18K1_i1jZ94B",
        "colab_type": "code",
        "outputId": "ebfafbc4-6c91-4f63-fbef-ce393c1583de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "nodes_df.createOrReplaceTempView('nodes')\n",
        "edges_df.createOrReplaceTempView('edges')\n",
        "\n",
        "friends_df = \\\n",
        "sqlContext.sql('select n1.user, n2.user as friend ' +\\\n",
        "               'from (nodes n1 join edges e on n1.nid = e.from) join nodes n2 on e.to = n2.nid')\n",
        "\n",
        "friends_df.show(5)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----------+\n",
            "|         user|     friend|\n",
            "+-------------+-----------+\n",
            "|newin_8425340|  newin_148|\n",
            "|newin_8423898|newin_57370|\n",
            "| newin_942965|newin_76756|\n",
            "| newin_215843|newin_76756|\n",
            "|newin_4674072|newin_76756|\n",
            "+-------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp5VkLumpYmT",
        "colab_type": "text"
      },
      "source": [
        "## Connecting Friends to Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9PkrJobXY0",
        "colab_type": "code",
        "outputId": "827d4ca2-f6eb-49ee-c794-8b0c71905af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "friends_df.createOrReplaceTempView('friends')\n",
        "\n",
        "sqlContext.sql('select u1.name.given_name as user, u2.name.given_name as friend '+\\\n",
        "               'from (linked_in u1 join friends on u1._id = user) join linked_in u2 on u2._id = friend').show(5)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------+\n",
            "|user|friend|\n",
            "+----+------+\n",
            "|Adam|  Abiy|\n",
            "|Abhi|Adukwu|\n",
            "+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4pmwWhbtlWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}