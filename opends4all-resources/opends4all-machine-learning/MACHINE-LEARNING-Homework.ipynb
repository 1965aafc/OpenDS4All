{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b9-vRrWoyhFR","nbgrader":{"grade":false,"grade_id":"cell-6aed0027f3a276a8","locked":true,"schema_version":1,"solution":false}},"source":["# Homework: K-Means Clustering\n","\n","In this homework, we will explore the k-means clustering algorithm using a BBC Sports dataset. As with all unsupervised learning problems, our goal is to discover and describe some hidden structure in unlabeled data. \n","\n","We will make the following **assumption**:  In the given data, the samples may be cleanly separated into k distinct groups over a set of features.\n","\n","The issue is that, while this assumption tells us that there are k distinct \"classes\" of samples, we know nothing about their content/properties.  If we could find samples that were representative of each of the k groups, then we could label the rest of the samples based on how similar they are to each of the representative or prototypical samples.  Armed with this intuition, the goal of the k-means clustering algorithm will be to find these representative samples (prototypes) through an iterative process.\n","\n","We can define such prototypes as either a:\n","- **centroid** – the average of similar points wrt continuous features (e.g. petal lengths)\n","- **medioid** – the most representative/most frequently occurring point wrt categorical features (e.g. discrete color labels, blue vs. red vs. green.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ysU5AnqAyhFS","nbgrader":{"grade":false,"grade_id":"cell-b711958939fdb75e","locked":true,"schema_version":1,"solution":false}},"source":["## Step 1: Download and unzip the dataset\n","\n","For this assignment you’ll use a dataset which clusters text articles from BBC Sports, by topic (athletics, cricket, football, rugby, tennis).  Download the preprocessed dataset from:\n","\n","http://mlg.ucd.ie/datasets/bbc.html\n","\n","(1) Download `bbcsport.zip` to your directory and (2) unzip bbcsport.zip into the four files contained therein. Leave these four files in your homework directory, i.e. do not put them in a data folder. You could also do this within your notebook using the `ZipFile` and `urlrequest.urlretrieve` packages. \n","\n","Look at the dataset's web page above to understand the role of each file.\n","\n","Part of the task here is just for you to make sense of the data, and practice being a data scientist.  A few remarks that might help you:\n","\n","* “Classes” are the five different topics, thus the class file may give a hint as to this (similarly with the docs file, which might be friendlier to read by a human).  Each document ID is ultimately mapped to a class.\n","* The mtx file represents a sparse matrix with counts of how often each word appeared in a document.  You may want to look at importing `scipy.io` and using `scipy.io.mmread` to read matrices, and the matrix `todense()` method to go from sparse to dense matrices.\n","* The ordering of the terms in the terms file implicitly gives you their word IDs."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install pandas\n","!pip install numpy\n","!pip install matplotlib\n","!pip install sklearn\n","!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"kShJchNyyhFT","nbgrader":{"grade":false,"grade_id":"cell-4753f8f4205080dc","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# Popular imports, you may modify this if you need to add things\n","\n","from urllib import request\n","import zipfile\n","import matplotlib\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.io\n","from sklearn import preprocessing\n","import nltk\n","\n","# For clustering\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.decomposition import PCA\n","from scipy.spatial.distance import cdist\n","from statistics import mode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"K_AWdIyqyhFV","nbgrader":{"grade":false,"grade_id":"cell-645c14b234728b0c","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# TODO: download http://mlg.ucd.ie/files/datasets/bbcsport.zip and unzip\n","#\n","# Look up the Python docs for ZipFile and urlrequest.urlretrieve\n","# (remember to use Python 3.x)\n","#\n","# Worth 0 points\n","\n","## YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"fO3_M-Y-yhFX","nbgrader":{"grade":true,"grade_id":"cell-9f8e4e881e6a2cf8","locked":true,"points":0,"schema_version":1,"solution":false}},"outputs":[],"source":["# Check that the files exist\n","import os.path\n","\n","if not os.path.isfile('bbcsport.classes'):\n","    raise ValueError(\"Don't appear to have successfully downloaded files\")\n","    \n","if not os.path.isfile('bbcsport.mtx'):\n","    raise ValueError(\"Don't appear to have successfully downloaded files\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BtsHKh_XyhFZ"},"source":["## Step 2 - Data Prep\n","\n","Load the data into a dataframe using any of the files you think are relevant. Columns should be terms and rows should be articles.\n","\n","You should ultimately create a `bbc_df` dataframe in which each row is an article, and each column represents the number of times a given word (term) appears.  The resulting dataset should have shape (737, 4613).  \n","\n","You should also read the class labels and turn them into a Pandas `Series` called `document_class` with column name `class_label`. This will be used at the end of the assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"RJy5yq48yhFa","nbgrader":{"grade":false,"grade_id":"cell-9872b0e6fec12c73","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# TODO: load the bbc_df and document_class\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":1485,"status":"ok","timestamp":1577813939342,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"0JxN_7rUyhFb","nbgrader":{"grade":true,"grade_id":"cell-06cb9e35076237ed","locked":true,"points":5,"schema_version":1,"solution":false},"outputId":"ff2f984e-bb14-4444-e555-d2b98efdf845"},"outputs":[],"source":["# Check that the shape of the dataset is correct.\n","\n","if bbc_df.shape[0] != 737 or bbc_df.shape[1] != 4613:\n","    raise ValueError('Unexpected shape of dataframe!')\n","bbc_df.shape"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GumhxGmhyhFf","nbgrader":{"grade":false,"grade_id":"cell-b353d35322931284","locked":true,"schema_version":1,"solution":false}},"source":["## Step 3 - K-Means\n","\n","Now the question is -- how many clusters should we use?  We hinted above that the BBC dataset classified documents into five classes, but can we validate that the data supports one cluster per class?\n","\n","First, turn the `bbc_df` into a matrix which we'll just call `X`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"8aLS_aDhyhFf","nbgrader":{"grade":false,"grade_id":"cell-18b3537f8bf0bef5","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# TODO: convert the dataframe to a matrix X\n","\n","##YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"_1g8DhzUyhFh","nbgrader":{"grade":true,"grade_id":"cell-e863b3ef7be56f47","locked":true,"points":2,"schema_version":1,"solution":false}},"outputs":[],"source":["# Check that the shape of the matrix is correct\n","if X.shape != (737, 4613):\n","    raise ValueError('Expected something different than', X.shape)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Ns5OVDHyhFi","nbgrader":{"grade":false,"grade_id":"cell-190072425bf452d9","locked":true,"schema_version":1,"solution":false}},"source":["Recall that in class, we described running K Means and plotting the **distortion** (sum of squared error given Euclidean distance).  From that we can find the \"elbow\" indicating the best tradeoff between number of clusters and distortion.\n","\n","\n","### Step 3.1 Testing Cluster Distortion\n","\n","Define a function called `test_cluster_size` that iterates over possible cluster sizes from 2 to a `max_cluster` (inclusive) value.  The function should takes as input (1) the data as a matrix and (2) `max_cluster`.  It should return a list of scores, **where `max_cluster[i]` is the score for cluster size i**.  You should use `None` for cluster sizes 0 and 1.\n","\n","Internally, you’ll want to use KMeans from `sklearn.cluster` to cluster your data.  As we saw from the slide examples, you can call the `fit` method over the data to generate clusters.  In each call, use random initialization of the centroids, with `max_iter=300`, `random_state=0`, and `n_init=30`.  It uses the term `inertia` to refer to distortion.\n","\n","Also, you'll note from the `sklearn.cluster` documentation on __[KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)__:\n","\n","Attributes:\t\n","* cluster_centers_ : array, [n_clusters, n_features]\n"," Coordinates of cluster centers\n","\n","* labels_ :\n","Labels of each point\n","\n","* inertia_ : float\n","Sum of squared distances of samples to their closest cluster center.\n","\n","* n_iter_ : int\n","Number of iterations run.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"gg0ZMBfjyhFi","nbgrader":{"grade":false,"grade_id":"cell-6f113e9f997a4e41","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# TODO: write test_cluster_size\n","\n","## YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ReuRF56ZyhFk","nbgrader":{"grade":true,"grade_id":"cell-a54e77b1073b6ecb","locked":true,"points":5,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","results = test_cluster_size(X, 3)\n","if results[2] < 248200 or results[2] > 248300:\n","    raise ValueError(\"Expected 248288\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xjJFJJtPyhFl","nbgrader":{"grade":false,"grade_id":"cell-95034ca00d3a259a","locked":true,"schema_version":1,"solution":false}},"source":["### Step 3.2 Finding the Elbow, Try 1\n","\n","Plot the number of clusters (from 2 to 9) vs distortion.  Let `opt_k` be the `k` value which results in the smallest distortion value."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":58929,"status":"ok","timestamp":1577813996817,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"G6xLHLUnyhFm","nbgrader":{"grade":false,"grade_id":"cell-937aef8f5d832052","locked":false,"schema_version":1,"solution":true},"outputId":"9ede4483-c798-47ca-9247-1a42ca0fef1e"},"outputs":[],"source":["# TODO: Plot clusters from 2 to 9.  Hint: you can be more efficient\n","# by writing a reusable plot_clusters function\n","\n","## YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"n4D92_V9yhFn","nbgrader":{"grade":true,"grade_id":"cell-e7e1baf3599e952f","locked":true,"points":5,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","if opt_k < 2 or opt_k > 9:\n","    raise ValueError('Bad optimal k')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CD9P0IZzyhFo","nbgrader":{"grade":false,"grade_id":"cell-eb4ac2560773ef44","locked":true,"schema_version":1,"solution":false}},"source":["Now, if your data looked like ours, you probably didn't see a 100% convincing elbow on the data (though there might have been something that *could* be interpreted as an elbow).\n","\n","## Step 4: Feature Scaling\n","\n","Perhaps we have odd feature scaling along some dimensions, because some words are more frequent than others.  Let’s see whether we should use the raw counts (as above), min-max scaling, Boolean value indicating appears/doesn’t appear, or log-scaled values.\n","\n","### Step 4.1. Scaling by min-max\n","\n","Given an input matrix X, we can rescale each feature along its min/max value range, as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"NvlVFv91yhFp","nbgrader":{"grade":false,"grade_id":"cell-7c81ce499ac793f8","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["X_copy = X.copy()\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_scaled = min_max_scaler.fit_transform(X_copy)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yTmOHhWQyhFq","nbgrader":{"grade":false,"grade_id":"cell-52c3298434acd8b3","locked":true,"schema_version":1,"solution":false}},"source":["Now plot the number of clusters (from 2 to 8) vs distortion.  Can you find a clear “elbow” here?  Set `opt_k` to the number of clusters you think represents the elbow."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":86200,"status":"ok","timestamp":1577814024107,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"IV6f9xQ8yhFr","nbgrader":{"grade":false,"grade_id":"cell-b08de8b523c7a478","locked":false,"schema_version":1,"solution":true},"outputId":"6b1144ae-e608-4219-8ea8-d876ec8bb95c"},"outputs":[],"source":["# TODO: Use scaled frequencies rather than raw frequencies to evaluate the distortion.\n","\n","## YOUR SOLUTION HERE\n","\n","opt_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"OICYBWGjyhFs","nbgrader":{"grade":true,"grade_id":"cell-3d5158434af1f128","locked":true,"points":3,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","if opt_k <= 1 or opt_k > 8:\n","    raise ValueError('Bad optimal k')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fSpSiDHcyhFt","nbgrader":{"grade":false,"grade_id":"cell-d16d7f7071ea6566","locked":true,"schema_version":1,"solution":false}},"source":["### Step 4.2. Boolean present/absent\n","\n","As an alternative, try replacing all elements of the matrix with 1 if they are nonzero (make sure to use Pandas functions, no loops). This corresponds to the Boolean case.  Make this a matrix `X_binary`.\n","\n","Again, plot the number of clusters (from 2 to 8) vs distortion.  Find the “elbow” after which the change in distortion tapers off notably.  Set `opt_k` to this.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":133548,"status":"ok","timestamp":1577814071466,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"2HlqY3q0yhFu","nbgrader":{"grade":false,"grade_id":"cell-3abdac2b6043c112","locked":false,"schema_version":1,"solution":true},"outputId":"55569660-8b4a-4d85-ea56-9737016fdc48"},"outputs":[],"source":["# TODO: Convert to Boolean values depending on whether there are 0s or non-0s.\n","\n","## YOUR ANSWER HERE\n","\n","opt_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"mr_PAhX4yhFv","nbgrader":{"grade":true,"grade_id":"cell-d8f8808ea76f5a0a","locked":true,"points":5,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","if opt_k < 2 or opt_k >= 9:\n","    raise ValueError('Bad optimal k')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xZnLvT_yyhFy","nbgrader":{"grade":false,"grade_id":"cell-36739313a31f5d1b","locked":true,"schema_version":1,"solution":false}},"source":["### Step 4.3. Log scaling\n","\n","As yet another alternative, try adding 1 to all elements in the original matrix and then take the numpy log of all elements in the original matrix. This is the NLP-inspired version, since word frequencies are informative but follow a Zipfian distribution. Call this matrix `X_log`.\n","\n","Again, plot the number of clusters (from 2 to 8) vs distortion.  Find the “elbow” after which the change in distortion tapers off notably.  Set `opt_k` to this."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":177758,"status":"ok","timestamp":1577814115687,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"5vOuUVH6yhFy","nbgrader":{"grade":false,"grade_id":"cell-857c87f9e81f4ac3","locked":false,"schema_version":1,"solution":true},"outputId":"b3b61901-3101-48bf-e6a7-e9134d530825"},"outputs":[],"source":["# TODO: Use log frequencies rather than raw frequencies to evaluate the distortion.\n","\n","##YOUR ANSWER HERE\n","\n","opt_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"MSb0WmoMyhF0","nbgrader":{"grade":true,"grade_id":"cell-c82ee95def1e2355","locked":true,"points":3,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","if opt_k < 2 or opt_k >= 9:\n","    raise ValueError('Bad optimal k')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"imaVsrW4yhF1","nbgrader":{"grade":false,"grade_id":"cell-a43100ffb33fcf99","locked":true,"schema_version":1,"solution":false}},"source":["## Step 5 The Variance Ratio Criterion\n","\n","We can also compute a different distortion metric called the Variance Ratio Criterion ($VRC$), given by\n","\n","$$ VRC(k) = \\frac{SS_B}{k-1} / \\frac{SS_W}{N - k}$$\n","\n","where $SS_B$ is the sum of squared distance between the cluster centers and the grand mean (calculated per data point), $k$ is the number of clusters, $SS_W$ is the sum of square distance between data points and their assigned cluster centers, and $N$ is the number of data points.\n","\n","\n","### Step 5.1 The Grand Mean\n","\n","As a quick check, compute the grand mean of the original dataset in the cell below. Store it in the variable `grand_mean` so we can test it. Do not use a loop."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":337,"status":"ok","timestamp":1577814332529,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"hLBTw1cpyhF1","nbgrader":{"grade":false,"grade_id":"cell-adf966a91cd00c85","locked":false,"schema_version":1,"solution":true},"outputId":"957b9a89-11a4-4b7d-fba9-56ef89fe9c92"},"outputs":[],"source":["# Compute the grand mean \n","\n","##YOUR ANSWER HERE\n","\n","grand_mean"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j8cL7oGlyhF4","nbgrader":{"grade":false,"grade_id":"cell-5d0bb51f5adfabd3","locked":true,"schema_version":1,"solution":false}},"source":["### Step 5.2 Implementing VRC\n","\n","Write a function `test_vrc(data, max_num_cluster)` that computes the $VRC$ on clusterings of size 2 up to size `max_num_cluster`. Since we are passing in the data, compute a new grand mean within the function. However, since the grand mean does not depend on the clusters, you should not compute it within a loop. Note that $SS_W$ is the distortion metric that you used in Steps 1.3 and 1.4.\n","\n","**Using the original version of your data, so not the scaled, binary, or log versions**, plot the number of clusters (from 2 to 8) vs $VRC$.  Set `opt_k` to the number of clusters recommended by $VRC$. Note that you are now looking for the first or highest local maximum and not looking for an elbow."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":218493,"status":"ok","timestamp":1577814156446,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"9_lhMtMtyhF4","nbgrader":{"grade":false,"grade_id":"cell-5601d099896e1282","locked":false,"schema_version":1,"solution":true},"outputId":"617b1d03-8b57-462d-a472-9b90cd7d3c5b"},"outputs":[],"source":["# TODO: plot VRC and find opt_k\n","\n","##YOUR ANSWER HERE\n","\n","opt_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Uu9jemMuyhF7","nbgrader":{"grade":true,"grade_id":"cell-1d6e3b92b4e93330","locked":true,"points":5,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","if opt_k < 2 or opt_k >= 9:\n","    raise ValueError('Bad optimal k')\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uf0lJT40yhF8","nbgrader":{"grade":false,"grade_id":"cell-5ced9e729fd7ebc0","locked":true,"schema_version":1,"solution":false}},"source":["## Step 6: Dimensionality Reduction via PCA\n","\n","Maybe we don’t need every word to be a separate dimension.  Let’s use PCA to find the most important features.  We're going to create a PCA model with 700 components, fit it to our scaled data, and then plot how much each feature (cumulatively) affects the variance, all with this code."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":220317,"status":"ok","timestamp":1577814158281,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"9MWekkAmyhF8","nbgrader":{"grade":false,"grade_id":"cell-026f9ad1c3bfc49f","locked":false,"schema_version":1,"solution":true},"outputId":"63653750-fc29-47f7-b2b3-ceb03857151c"},"outputs":[],"source":["# This part is just to free up some memory\n","bbc_df = pd.DataFrame({})\n","\n","X_forPCA = X_scaled.copy()\n","pca_model = PCA(n_components = 700)\n","pca_model.fit(X_forPCA)\n","variance = pca_model.explained_variance_ratio_ #calculate variance ratios                                         \n","\n","var=np.cumsum(np.round(pca_model.explained_variance_ratio_, decimals=3)*100)\n","var #cumulative sum of variance explained with [n] features                                                          \n","\n","plt.ylabel('% Variance Explained')\n","plt.xlabel('# of Features')\n","plt.title('PCA Analysis')\n","plt.style.context('seaborn-whitegrid')\n","plt.plot(var)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ukBCLTwDyhF-","nbgrader":{"grade":false,"grade_id":"cell-b0e1b7b04539b96b","locked":true,"schema_version":1,"solution":false}},"source":["In the cell below, assign a value to `cutoff` (to the nearest multiple of 50) for an appropriate number of PCA dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"PAuV_SJGyhF-","nbgrader":{"grade":false,"grade_id":"cell-86b54d61de83a87f","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# Assign a value to cutoff, to the nearest multiple of 50\n","\n","##YOUR ANSWER HERE\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4swVmOzryhGB","nbgrader":{"grade":false,"grade_id":"cell-350381394a7dfa8a","locked":true,"schema_version":1,"solution":false}},"source":["Re-run PCA with `n_components` equal to the cutoff value, and run `fit_transform` on `X_forPCA` to get an updated feature matrix. Now plot the number of clusters (from 2 to 12) vs $VRC$ using the PCA-transformed data and assign the best `k` to `opt_k`. Remember, you are looking for a local maximum."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"colab_type":"code","executionInfo":{"elapsed":228927,"status":"ok","timestamp":1577814166905,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"lgmhTCO1yhGC","nbgrader":{"grade":false,"grade_id":"cell-e29180d2255b424a","locked":false,"schema_version":1,"solution":true},"outputId":"3cdcb0e7-f4e7-45a1-8b43-892fd71f64a1"},"outputs":[],"source":["# TODO: set X_PCA\n","\n","##YOUR ANSWER HERE\n","\n","opt_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":228923,"status":"ok","timestamp":1577814166906,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"wNB4_U-SyhGD","outputId":"df931132-3521-4204-b57d-142737826a2d"},"outputs":[],"source":["X_PCA.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"3evPYYpCyhGF","nbgrader":{"grade":true,"grade_id":"cell-0326d99e2d22ddc4","locked":true,"points":5,"schema_version":1,"solution":false}},"outputs":[],"source":["# Sanity check\n","assert(X_PCA.shape == (737,cutoff))\n","\n","if opt_k < 2 or opt_k >= 9:\n","    raise ValueError('Bad optimal k')\n","    "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7b4lQ-qhyhGG","nbgrader":{"grade":false,"grade_id":"cell-053b8a80a485ae9c","locked":true,"schema_version":1,"solution":false}},"source":["## Step 7: Evaluating Cluster Results\n","\n","Now we are going to compare the true labels with the clusters. Note that the clusters are numbered 0 to $k-1$, but even if $k = 5$, these numbers are likely not in the same order as (athletics, cricket, football, rugby, tennis). So, write a function `evaluate_clusters(data, cls, k)` that does the following:\n","\n","1. Cluster `data` into `k` clusters. `cls` should contain the same number of rows as data, but contain the indices of the true labels in the column `class_label`.\n","1. Determine the index of the true label associated with each cluster.  To do this, you’ll need to compute the index of the most common true label in each cluster. Use the `mode` function that you have already imported.\n","2. Map the output of the clustering to the integers that you found. You could print this mapping for extra insight, but this is not required.\n","2. Count the number of correctly classified articles and return the proportion of correctly classified articles.\n","\n","For extra insight, you could use `nltk.FreqDist` to print the size of each cluster, but this is not required."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"-jzJoUpjyhGG","nbgrader":{"grade":false,"grade_id":"cell-18959992e3fbc3f2","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# TODO: write function evaluate_clusters(data, cls, k)\n","\n","##YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"colab_type":"code","executionInfo":{"elapsed":235090,"status":"ok","timestamp":1577814173089,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"UP5u26dCyhGH","nbgrader":{"grade":true,"grade_id":"cell-2f7f644c81d137d6","locked":true,"points":5,"schema_version":1,"solution":false},"outputId":"6d35ed2e-5a01-4822-f1c9-7224ebd4a77a"},"outputs":[],"source":["baseline_accuracy = evaluate_clusters(X, document_class, 5)\n","print(baseline_accuracy)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wMaUa2u5yhGI","nbgrader":{"grade":false,"grade_id":"cell-0ed09338d07d95b1","locked":true,"schema_version":1,"solution":false}},"source":["Among your 4 versions of the dataset and choices of number of clusters between 2 and 8, find the best configuration and report your accuracy value. Store the best version of your data as `opt_data` and the best $k$ as `opt_k`. Rather than posting on Piazza about the \"right answer\", please take this opportunity to explore. The test cases for this part will accept more than the absolute best configuration. You just have to be pretty close."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"cA7r3nRHyhGJ","nbgrader":{"grade":false,"grade_id":"cell-396dcb86429ffc94","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["# Set opt_data and opt_k in this cell.\n","\n","##YOUR ANSWER HERE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"colab_type":"code","executionInfo":{"elapsed":243292,"status":"ok","timestamp":1577814181305,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"5itZ1JqDyhGK","nbgrader":{"grade":true,"grade_id":"cell-e853dd1fe07b8c1d","locked":true,"points":5,"schema_version":1,"solution":false},"outputId":"0f989936-ea36-48ce-982d-a2c9df77354c"},"outputs":[],"source":["best_accuracy = evaluate_clusters(opt_data, document_class, opt_k)\n","print(best_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":243287,"status":"ok","timestamp":1577814181306,"user":{"displayName":"Susan Davidson","photoUrl":"","userId":"06674934534309486107"},"user_tz":300},"id":"CFQDJ3CxyhGL","outputId":"af28f1eb-bc72-4025-c94e-fc26769d35f5"},"outputs":[],"source":["X.shape"]}],"metadata":{"anaconda-cloud":{},"celltoolbar":"Create Assignment","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Homework_with_Answers.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}