{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_w/o_sol.ipynb","provenance":[{"file_id":"1hAzA9488tEullm8tgul8LYBxfJ8VV7we","timestamp":1572377020797}],"collapsed_sections":[]},"dataset_inspector":{"cols":{"lenName":24,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"110px","left":"1185px","right":"20px","top":"108px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"qzbDVOuhZCo1","nbgrader":{"cell_type":"markdown","checksum":"57127bfeccebf7eb7e910658b52aeb27","grade":false,"grade_id":"cell-3a1d9a996b627af7","locked":true,"schema_version":3,"solution":false}},"source":["# Homework: Data Wrangling and Cleaning\n","\n","We all know that cryptocurrencies are all the rage today.  Could we train an algorithm to tell the difference between a webpage about cryptocurrency and a webpage about something else?\n","\n","This initial assignment goes over some of the basic steps in (1) acquiring data from the web, (2) acquiring tabular data, (3) cleaning and linking data, and (4) training a simple machine learning classifer.  Along the way you'll learn a few of the basic tools, and get a very basic understanding of one way to represent documents.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V5CO9AOBUK-4","colab":{}},"source":["# Standard pip install...  Put all of your to-install packages here.\n","# Depending on your configuration, you may need to change pip3 to pip\n","!pip3 install scrapy\n","!pip3 install lxml\n","!pip3 install scikit-learn\n","!pip3 install swifter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XsqKccaWUK-7","colab":{}},"source":["# Standard imports; it's cleaner to put them here so they can be used\n","# throughout the notebook\n","\n","import pandas as pd\n","import numpy as np\n","from lxml import etree\n","import sqlite3\n","import swifter\n","import urllib\n","import re\n","\n","import nltk\n","from nltk import classify\n","from nltk import NaiveBayesClassifier\n","from nltk.stem import PorterStemmer\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"2d_SGrC0UK-8","nbgrader":{"cell_type":"markdown","checksum":"dc29aa53c704cada7f6e10ecf817db11","grade":false,"grade_id":"cell-2ae3cd0ba764bf75","locked":true,"schema_version":3,"solution":false}},"source":["## Task 1: Acquiring data for training our system\n","\n","First let's get some information about what's a cryptocurrency.  For that -- there's always [Wikipedia](https://en.wikipedia.org/wiki/List_of_cryptocurrencies)!\n","\n","But of course it won't give us the data exactly the way we want it, so we'll need to do a bit of information extraction and data wrangling. We will also try to get current price levels from [Yahoo](https://finance.yahoo.com/cryptocurrencies)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"sJd7yCmSUK-9","nbgrader":{"cell_type":"markdown","checksum":"eba4f09eac3a10522253648aee781e87","grade":false,"grade_id":"cell-b8d0a5f521360756","locked":true,"schema_version":3,"solution":false}},"source":["### Task 1.1: Fetch the list of pages from Wikipedia and put it into a dataframe\n","\n","First we'll get the master table of \"known\" cryptocurrencies. Use the `read_html()` function from `pandas`. \n","\n","**Notes for downloading webpage**: In order to keep consistency, we crawled the webpages for you. Instead of visiting the original page, you should visit webpage that is consists of `${prefix}/${website}`. For example, if you want to visit https://en.wikipedia.org/wiki/List_of_cryptocurrencies, you should actually enter https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/en.wikipedia.org/wiki/List_of_cryptocurrencies. The default prefix is `https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/`. If your instructor have another url base for you then you should follow the instructions instead. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"GinY-jklZTuc","nbgrader":{"cell_type":"code","checksum":"a8bb18ff7ac1ba34fabb39c5f4e4a9c5","grade":false,"grade_id":"1-1-1-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO:\n","# (1) Fetch files from our Wikipedia wikipedia crawl: \n","#     Notice that to read https://en.wikipedia.org/wiki/List_of_cryptocurrencies you can visit the following crawled page: \n","#     https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/en.wikipedia.org/wiki/List_of_cryptocurrencies \n","# (2) Parse into a dataframe called cryptocurrency_df\n","\n","# YOUR CODE HERE\n","\n","display(cryptocurrency_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"dKvFl0yfikvN","nbgrader":{"cell_type":"markdown","checksum":"6328beb59b0529da85a1bcbcce280c0f","grade":false,"grade_id":"cell-4774f393116f5d10","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Next, do the same for the following two sites. Yahoo gives a maximum of 100 prices at a time, so this is why we have to have two queries."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"hPCMExmNUK-_","nbgrader":{"cell_type":"code","checksum":"961bcad9a4eae11a5131f86ccc9e3a69","grade":false,"grade_id":"1-1-2-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Make two price dataframes from our crawled dataset corresponds to the following page of the query result: \n","# price_1_df: https://finance.yahoo.com/cryptocurrencies/?count=100&offset=0\n","# price_2_df: https://finance.yahoo.com/cryptocurrencies/?count=100&offset=100\n","# Hint: special characters used for parameter passing are encoded by GitHub, like ?, =, &. You should take care of the translation. \n","# e.g. the first webpage is actually https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/finance.yahoo.com/cryptocurrencies/%3Fcount%3D100%26offset%3D0\n","\n","# YOUR CODE HERE\n","\n","price_df = price_1_df.append(price_2_df)\n","\n","display(price_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"VSSXMhqSUK_B","nbgrader":{"cell_type":"code","checksum":"4c7f290f0a7682c1c857c163b40692d2","grade":true,"grade_id":"1-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["# Quick sanity check 1.1 for cryptocurrency_df: does it have the columns from the Wikipedia table?\n","\n","if not 'Currency' in cryptocurrency_df:\n","    raise AssertionError('Expected column called \"Currency\"')\n","    \n","if not 'Founder(s)' in cryptocurrency_df:\n","    raise AssertionError('Expected column called \"Founder(s)\"')\n","\n","display(cryptocurrency_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"2P0K5eVJUK_E","nbgrader":{"cell_type":"markdown","checksum":"82866152a78acf7786154ee380d252b2","grade":false,"grade_id":"cell-e102b6da0afa92b9","locked":true,"schema_version":3,"solution":false}},"source":["### Task 1.2 First bit of data Cleaning:  Clean up the schema names.\n","\n","It turns out that SQL databases often don't like parentheses and spaces in the column names.  Change the column names for the appropriate columns, by \n","\n","1. removing the parts in parentheses\n","2. trimming any blank spaces before or after the names\n","3. inserting underscores for spaces.  \n","4. drop unnamed columns which might be introduced by mistable\n","\n","Hint: there are functions called `trim`, `strip`, `find`, `replace`."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"9gs7SsqkUK_F","nbgrader":{"cell_type":"code","checksum":"f717c4ceb1525a196b5f6dbdc32667bf","grade":false,"grade_id":"1-2-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO:\n","# For all column names in cryptocurrency_df, \n","# (1) remove anything in parentheses, \n","# (2) remove leading and trailing spaces, \n","# (3) replace remaining spaces with underscores\n","# (4) remove any unnamed column with null values that are introduced by error\n","\n","# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"2BUupYbJUK_G","nbgrader":{"cell_type":"code","checksum":"3da1adf754aa89780fce9e905bf692a5","grade":true,"grade_id":"1-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["# Sanity check 1.2 for cryptocurrency_df\n","\n","for column in cryptocurrency_df.keys():\n","    if column.find(' ') >= 0:\n","        raise AssertionError('Forgot to remove a space in \"%s\"'%column)\n","    elif column.find('(') >= 0 or column.find(')') >= 0:\n","        raise AssertionError('Forgot to remove a paren in %s'%column)\n","        \n","display(cryptocurrency_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"nAhUmsLkXwki","nbgrader":{"cell_type":"markdown","checksum":"b569bf46e7697a9509f3d8da0ccb8dee","grade":false,"grade_id":"cell-f43f3351c585fae7","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 1.3: Joining the tables\n","\n","We are now going to try to put these two sources of information into one table. The requirement is that we want to make sure that we have an entry for every currency in the Wikipedia list, but not necessarily for every currency in the Yahoo price list. Of the four types of join, two can achieve this requirement. For extra practice, see if you can figure out both correct answers."]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"lyL-QyurHZk5","nbgrader":{"cell_type":"markdown","checksum":"2ff0bac8b2840dc04a589a6ca5e331f6","grade":false,"grade_id":"cell-7731a5ca51f701cc","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["#### Task 1.3.1 Attempt #1\n","\n","In the cell below, join `cryptocurrency_df` and `price_df` using \"Name\" as the join index of `price_df` and \"Currency\" as the join index of `cryptocurrency_df`. The result should be named `joined_on_name_df`. Do not make any changes to the data frames yet, even though you may see a problem with joining them now."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"T0l2d7cI4mTZ","nbgrader":{"cell_type":"code","checksum":"4802ba43c67ea0c19d654156dca30677","grade":false,"grade_id":"1-3-1-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Join cryptocurrency_df and price_df\n","\n","# YOUR CODE HERE\n","\n","display(joined_on_name_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"-XBBxAANah8Q","nbgrader":{"cell_type":"code","checksum":"abe2f2f7f530e194a17ec03c5898f356","grade":true,"grade_id":"1-3-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 1.3.1 for joined_on_name_df\n","\n","if len(joined_on_name_df.columns) != 20:\n","    raise AssertionError('Your joined table has %d columns, an unexpected number.'%len(joined_on_name_df.columns))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"dtD8KovehXDW","nbgrader":{"cell_type":"markdown","checksum":"15d8ebba7b79db4537f6643886d37807","grade":false,"grade_id":"cell-05ab54359c83552b","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["#### Task 1.3.2 Cleaning up the names\n","\n","You may have noticed a mismatch for how the currencies are named between the two data frames. Use the `apply` function to replace the values in the `price_df[\"Name\"]` column so they better match the values in `cryptocurrency_df[\"Currency\"]`.\n","\n","Then rerun your join from 1.3.1 and name it the same way."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"5gc1aizt5DHN","nbgrader":{"cell_type":"code","checksum":"4b0be3955ca0a9a9be7c2fce4ac61e24","grade":false,"grade_id":"1-3-2-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Remove Fix Name column in price_df and redo the join\n","\n","# YOUR CODE HERE\n","\n","display(joined_on_name_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"d6vi4TjHki1L","nbgrader":{"cell_type":"code","checksum":"98bdf1beea84d6c53bea894c6bbdc4ab","grade":true,"grade_id":"1-3-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 1.3.2 for joined_on_name_df\n","\n","if len(joined_on_name_df[joined_on_name_df[\"Name\"].notna()]) == 0:\n","    raise AssertionError('Your join did not find any matches. Maybe you did something wrong?')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"YQw49TttUK_K","nbgrader":{"cell_type":"markdown","checksum":"385974619367955c4f6fbbfc60e211f4","grade":false,"grade_id":"cell-e2ba62e3ef673b7e","locked":true,"schema_version":3,"solution":false}},"source":["#### Task 1.3.3: Clean the citations out of the content.\n","\n","As we saw in lecture, the html processing function converts Wikipedia citations to normal text. You may have noticed that this is keeping at least one of the cryptocurrencies from matching during the join. In the cell below, use `applymap` to remove these citations from the entire `cryptocurrency_df` table. Assume that every instance of \"`[`\" begins a citation. In this case only, it is okay if you delete everything after the \"`[`\", including the stuff after \"`]`\".\n","\n","Then rerun your join from 1.3.2 and name it the same way. Did you get more matches?"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"yJvV8EHRUK_K","nbgrader":{"cell_type":"code","checksum":"8e374581414b8734b63e22bb0c12b187","grade":false,"grade_id":"1-3-3-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO: Remove citations\n","\n","# YOUR CODE HERE\n","\n","display(joined_on_name_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"Dxxobixjo9FK","nbgrader":{"cell_type":"code","checksum":"0b378d0669b965f6fb9c8f44792fdb60","grade":true,"grade_id":"1-3-3-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 1.3.3 for joined_on_name_df\n","\n","print(\"%d matches found\"%len(joined_on_name_df[joined_on_name_df[\"Name\"].notna()]))\n","if len(joined_on_name_df[joined_on_name_df[\"Name\"].notna()]) == 0:\n","    raise AssertionError('Your join did not find any matches. Maybe you did something wrong?')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"nXI3K1gzlkf6","nbgrader":{"cell_type":"markdown","checksum":"fa9856f9a3141de682cff5f5440649ea","grade":false,"grade_id":"cell-25c55ba9e3bbb3eb","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["#### Task 1.3.4 A Better Column\n","\n","Look again at `cryptocurrency_df` and `price_df` and select better columns for indexing the join. Consider an `apply` function for the relevant column in `cryptocurrency_df` and for the relevant column in price_df` that you select. \n","\n","Name this table `joined_df`. To get the points for this section, you need to match at least as many currencies as our solution."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"bdy82NVn8JCe","nbgrader":{"cell_type":"code","checksum":"b4fd5cf9cd7892dfe7639989f871a6c3","grade":false,"grade_id":"1-3-4-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Improve the join by switching to different columns\n","\n","# YOUR CODE HERE\n","\n","# display(joined_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"yEwuI4AwUK_M","nbgrader":{"cell_type":"code","checksum":"504cd75c6ffa763d331b45b00ffb6417","grade":true,"grade_id":"1-3-4-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["# Sanity check 1.3.4 for joined_df\n","\n","print(\"%d matches found\"%len(joined_df[joined_df[\"Name\"].notna()]))\n","if len(joined_df[joined_df[\"Name\"].notna()]) <= len(joined_on_name_df[joined_on_name_df[\"Name\"].notna()]):\n","    raise AssertionError('Your new join is not better than the old one. Maybe you did something wrong?')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"SyIpJioCUK_O","nbgrader":{"cell_type":"markdown","checksum":"fd318eacc88cd523940f311a3ba569d8","grade":false,"grade_id":"cell-2d67034b9182d155","locked":true,"schema_version":3,"solution":false}},"source":["### Task 1.4: Save the cryptocurrency list in a database table\n","\n","We don't want to continue to hit Wikipedia.org every time we want to consult the list of cryptocurrencies.  Save your `cryptocurrency_df` to sqlite, in a table called `cryptocurrency`.  \n","\n","**The Dataframe `index` has no particular meaning, so don't save it!**"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"lBouXUwPUK_P","nbgrader":{"cell_type":"code","checksum":"17a26be73399ec67314b2e4d8e407bcb","grade":false,"grade_id":"1-4-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO: convert cryptocurrency_df to sqlite\n","\n","conn = sqlite3.connect('local.db')\n","\n","# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"OxxmLlveUK_Q","nbgrader":{"cell_type":"code","checksum":"d27749b16eca3815c502d1931efb8de8","grade":true,"grade_id":"1-4-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["# Sanity check 1.4 for sqlite databases\n","\n","crypto2 = pd.read_sql_query('select * from cryptocurrency', conn)\n","\n","if 'index' in crypto2:\n","    raise AssertionError('Please disable the index, since it isn\\'t important information')\n","    \n","display(crypto2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"hQj8kQDZUK_S","nbgrader":{"cell_type":"markdown","checksum":"8c79b0870a69c620e5db812efd2b9125","grade":false,"grade_id":"cell-2d7887a3456d8c6e","locked":true,"schema_version":3,"solution":false}},"source":["### Task 1.5: Read the cryptocurrency pages\n","\n","Now let's take each of the cryptocurrency names and find the associated URL. The names of the currencies were originally clickable links on the [webpage](https://en.wikipedia.org/wiki/List_of_cryptocurrencies) that we made the table from, but unfortunately, `pandas` automatically deleted the URLs. So we have to regenerate them. Feel free to look at that page to see what the correct URL is for each currency.\n","\n","In the cell below, complete the function `crawl`. The function name, inputs, first line, and last line are provided for you. \n","\n","`list_of_urls` should contain the URLs of interest as a list, column of a pandas DataFrame, or some other iterable over strings. \n","\n","`prefix` contains a common string that should be added to the beginning every URL in `list_of_urls` before each URL is queried. \n","\n","The line `pages = {}` creates an empty dictionary. After running your part of the function `crawl`, `pages` should have currency names as its keys and the corresponding Wikipedia page contents as its values. This is what the function returns.\n","\n","You have two options for completing this cell:\n","\n","1. If you want to use `urllib.request.urlopen`, you should then use `read()` and `decode('utf-8')`.\n","\n","2. If you want to use `scrapy`, follow the process in [this notebook from class](https://www.google.com/url?q=https://drive.google.com/file/d/1VfnlGr_VofdcEqACM2jRu2BwYm0QyTSh/view?usp%3Dsharing&sa=D&ust=1567968915286000&usg=AFQjCNG5iEWgUoA3DrRLhV1TKiT2OXHD1A).\n","\n","For now, use a `try` statement to catch the errors and print a message that the URL could not be crawled. That is, in this cell we will have a **single rule** and not do any manual cleaning.  If you were doing this at web scale, you would be reluctant to invest a lot of manual effort..."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"KWhAXKtIUK_S","nbgrader":{"cell_type":"code","checksum":"e814b0d6adeaa837ce438ef3200fcc3c","grade":false,"grade_id":"1-5-1-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO: Crawl the pages.  \n","# Trap the errors and figure out what you need to fix (in the cleaning step below)\n","\n","def crawl(list_of_urls, prefix=\"\"):\n","    pages = {}\n","# YOUR CODE HERE\n","    return pages"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"DyXjnGwJd_3S","nbgrader":{"cell_type":"markdown","checksum":"817602c34603b02a17f289d644cec608","grade":false,"grade_id":"cell-3723646eda1946ab","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["The following cell passes the currencies in our table to the `crawl` function. You do not need to modify the cell."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"sOV0Ic68UK_U","nbgrader":{"cell_type":"code","checksum":"5e18909ed5d515c265f72842e2783385","grade":true,"grade_id":"1-5-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["# Sanity check 1.5.1 for initial crawl\n","\n","pages = crawl(cryptocurrency_df['Currency'], 'https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/en.wikipedia.org/wiki/')\n","for page in pages:\n","    print (page)\n","    \n","print ('Total crawl: %d cryptocurrencies'%len(pages))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"zwcnmQUhUK_X","nbgrader":{"cell_type":"markdown","checksum":"1f19f03e916ba2c96fad600508a89aaa","grade":false,"grade_id":"cell-deb0ffb9e533fb7a","locked":true,"schema_version":3,"solution":false}},"source":["Did you get any errors? Did you ever get the wrong URL (and therefore the content from the wrong page)? Fix those two problems in the function `crawl_better` below. This function has the same inputs and outputs as `crawl`, but this time, it is okay if your fixes are specific to these sites. For example, you can try attaching `_(disambiguation)`, pull up that page's `etree.HTML(content)` and look for a link that has the name of the currency plus `' (cryptocurrency)'`."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"LE2T3xjPUK_Y","nbgrader":{"cell_type":"code","checksum":"063fe1a4e9b0162282296f114a5ec4ae","grade":false,"grade_id":"1-5-2-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO: Re-run the crawl, fixing the issues\n","\n","# Crawl the pages.  You may use urllib.request.urlopen or scrapy\n","# Assemble the list of results in the list pages.\n","# Trap the errors and figure out what you need to fix (in the cleaning step below)\n","\n","def crawl_better(list_of_urls, prefix=\"\"):\n","    pages = {}\n","# YOUR CODE HERE\n","\n","    return pages"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"89GQIhVegj8u","nbgrader":{"cell_type":"markdown","checksum":"9f0af79e6101466ffaa7be8bfbd185ab","grade":false,"grade_id":"cell-63b0ed2d452d5c9d","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["As before, the cell below just runs your function and does not need to be modified."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"02DhGSKtgf1I","nbgrader":{"cell_type":"code","checksum":"a1e928ecf6b4f0a2d7ebe05283816eca","grade":true,"grade_id":"1-5-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 1.5.2 for better crawl\n","\n","pages = crawl_better(cryptocurrency_df['Currency'], 'https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/en.wikipedia.org/wiki/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dqI_camUUK_c"},"source":["### Task 1.6: Sanity-check and fix\n","\n","Note that sometimes terms in Wikipedia are **ambiguous**, so just following the page doesn't always get what you want.  The Wikipedia page for [Tether](https://en.wikipedia.org/wiki/Tether) does not describe a cryptocurrency.\n","\n","We can add a data-cleaning rule to check this: every cryptocurrency should mention the term \"blockchain\".  Here's a sanity check you can use.  If there are any disambiguation pages, you need to go back to Task 1.5 and update your process to crawl the right page.\n","\n","You do not need to modify this cell."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"wbd9_N5OUK_d","nbgrader":{"cell_type":"code","checksum":"d31d9334ec337b3752a857f241eef6f8","grade":true,"grade_id":"1-6-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["count_wrong = 0\n","\n","for page,content in pages.items():\n","    if isinstance(content, bytes):\n","        raise AssertionError('Please run decode(\\'utf-8\\') on the content to decode to a string')\n","        content = content.decode('utf-8')\n","        \n","    if 'blockchain' not in content:\n","        print(page + ': ' + ' -- did not find blockchain!')\n","        count_wrong = count_wrong + 1\n","\n","        \n","print ('Total crawl: %d cryptocurrencies'%len(pages))\n","\n","if count_wrong > 0:\n","    raise AssertionError('Need to follow Wikipedia disambiguation pages on %d items!'%count_wrong)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"zHSdtQFwNDxe","nbgrader":{"cell_type":"markdown","checksum":"a650984d4c28f12b62877198869cbc40","grade":false,"grade_id":"cell-1e1c896f1e786b55","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 1.7: Clean the articles\n","\n","So far, we have captured HTML content for each Wikipedia article, but HTML is not very easy to read and process. So the next step is to clean up the text in each article. To do that, you need to complete the function definition below. The function name, and input are provided for you. \n","\n","The first step is to get a list of paragraphs of content. See our [slides](https://www.google.com/url?q=https://drive.google.com/a/seas.upenn.edu/file/d/163sCi0h5RJAXynE1Vo37bAQtOvcwW_wv/view?usp%3Dsharing&sa=D&ust=1567968915286000&usg=AFQjCNGDBY3SNFEJIh3m5k7GyYmhK2Q52w) on xpath for hints. Then, for each word (string between whitespace characters):\n","\n","1. Remove the leading and trailing whitespace using `strip()`\n","2. Remove the word entirely if it is only white space.\n","3. Remove the word entirely if it is only numerics (you may use `isnumeric()` to test for this).\n","\n","Finally join the words together into one string with spaces in between using `' '.join()`. The function should return that string (output)."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"v_7ihaxdUK_i","nbgrader":{"cell_type":"code","checksum":"071c9c06645d38dad2618e8db0cb5efe","grade":false,"grade_id":"1-7-ans","locked":false,"schema_version":3,"solution":true},"colab":{}},"source":["# TODO: Complete the clean_article function, as described above.\n","\n","def clean_article(content):\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"s-N_8GpJW36n","nbgrader":{"cell_type":"markdown","checksum":"787b3fbf9ee5774e813f83c303d4c017","grade":false,"grade_id":"cell-207fe106eb600a6a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["The following cell assembles our cleaned articles into a DataFrame. You do not need to modify the cell."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"A5mnrUVgUK_j","nbgrader":{"cell_type":"code","checksum":"be42569383ef2d8aa453f71cf8acdba3","grade":true,"grade_id":"1-7-sanity","locked":true,"points":1,"schema_version":3,"solution":false},"colab":{}},"source":["pages2 = []\n","for currency_name, content in pages.items():\n","    article = clean_article(content)\n","    pages2.append({'currency': currency_name, 'text': article})\n","\n","pages_df = pd.DataFrame(pages2)\n","\n","display(pages_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"CUVvKypxUK_n","nbgrader":{"cell_type":"markdown","checksum":"bedde93bec7a5fe09208ab99de4e6adb","grade":false,"grade_id":"cell-6e036ee47472985f","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Task 2: Build and run the classifier\n","\n","Now that we have the cryptocurrency articles processed, it is time to return to the original task of building a classifier that can identify cryptocurrency articles.\n","\n","## Task 2.1: Get the negative examples.\n","\n","If we want to build a (supervised) machine learning algorithm to detect content, we need both *positive* and *negative* examples.  In fact we want each successive training example to have an equal probability of being positive or negative.\n","\n","The following cell runs your `crawl` function from Task 1.5 and your `clean_article` function from Task 1.7. Note: We are using `crawl` not `crawl_better` because you may have included data-specific choices in `crawl_better` that are no longer true.\n","\n","You do not need to modify this cell."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"X0JB8G_mUK_o","nbgrader":{"cell_type":"code","checksum":"d4bb75b1a4e3ea045e9419e0c2e2f3aa","grade":true,"grade_id":"2-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["training = [\n","    'https://en.wikipedia.org/wiki/Tim_Cook',\n","    'https://en.wikipedia.org/wiki/The_Great_British_Bake_Off',\n","    'https://en.wikipedia.org/wiki/Google',\n","    'https://en.wikipedia.org/wiki/Chan_Zuckerberg_Initiative',\n","    'https://en.wikipedia.org/wiki/Politics',\n","    'https://en.wikipedia.org/wiki/Fake_news',\n","    'https://www.snopes.com/fact-check/social-media-hacker-warning/',\n","    'https://www.cnn.com/2019/08/31/us/dorian-animals-foster-release-wxc/index.html',\n","    'https://www.foxnews.com/us/indiana-dispatcher-helps-boy-who-called-911-with-fractions-homework',\n","    'https://www.usatoday.com/story/tech/talkingtech/2019/08/31/hello-iphone-11-new-features-we-want-apple-next-models/2153565001/',\n","    'http://theconversation.com/bury-fc-the-economics-of-an-english-football-clubs-collapse-122727',\n","    'https://fivethirtyeight.com/features/economists-are-bad-at-predicting-recessions/'\n","]\n","\n","current_prefix = 'https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/'\n","\n","rebase_training = [current_prefix + (currentpage[currentpage.find('//')+2 :  -1 if(currentpage[-1]=='/') else None]) for currentpage in training ]\n","\n","negative = crawl(rebase_training)\n","negative2 = []\n","for site, content in negative.items():\n","    article = clean_article(content)\n","    negative2.append({'site': site, 'text': article})\n","\n","negative_df = pd.DataFrame(negative2)\n","display(negative_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"MHF0VgEUUK_r","nbgrader":{"cell_type":"markdown","checksum":"4d07b6d3494f637f59051320b355fad4","grade":false,"grade_id":"cell-62cfcdeb3e015663","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 2.2: Process Document Text\n","\n","Right now, each Wikipedia article is a single string. This means, we only have one \"feature\" for the classifier. This is not enough. Tokenization (splitting up the article into words) would transform the data so that we have one feature per word. This probably would give us enough features to train a classifier.\n","\n","Complete the `get_words` function in the cell below. This function should take a string as input (the raw article).\n","\n","1. Create an empty list to store the good words.\n","\n","1. Break the article into sentences using the NLTK sentence tokenizer.\n","\n","1. Tokenize and part-of-speech tag each sentence.\n","\n","1. Run the provided `clean_word` function and Porter stemmer on each word.\n","\n","1. Finally, append the word stem to the list of good words if all of the following are true:\n","    1. The word stem is of nonzero length.\n","    2. The word stem has a length less than 20.\n","    3. The word stem is not a stopword.\n","    4. The word is a noun.\n","    5. The word stem is in `vocabulary`. Only apply this rule if `vocabulary` has nonzero length. It has zero length by default.\n","\n","6. Return the list of good words.\n","\n","To match our solution, it is important that you do these steps in the given order."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"eRBEZNakzHaE","nbgrader":{"cell_type":"code","checksum":"a5188b69df96f7eaaee813ecb68eb694","grade":false,"grade_id":"2-2-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Complete the get_words function\n","\n","sw = set(stopwords.words(\"english\"))\n","sw.add(\"'s\")\n","stemmer = PorterStemmer()\n","\n","def clean_word(word):\n","    word = word.lower()\n","    word2 = ''\n","    for w in word:\n","        if w.isalpha() or (len(word2) > 0 and w.isnumeric()):\n","            word2 = word2 + w\n","    return word2\n","\n","def get_words(article, vocabulary=[]):\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"2vxeAJBcAQBc","nbgrader":{"cell_type":"code","checksum":"21c6886fb673daa034fd56185b83d557","grade":true,"grade_id":"2-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.2 for getting the word stems from articles\n","\n","print(get_words(\"to be or not to be\"))\n","print(get_words(\"He wants to test the functionality of xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx in article 091019.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"N4H2uByrC1AA","nbgrader":{"cell_type":"markdown","checksum":"21ef74d1dabb994b0d49d4c60346dc69","grade":false,"grade_id":"cell-eb6d77467ec87070","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 2.3 Train the classifier\n","\n","Adapt the code from the NLTK lecture notebook to complete the `build_classifier` function. This function takes as input the two column dataframes `positive_df` and `negative_df`, and also an optional vocabulary list. It should run `get_words` on each article in each dataframe, get a frequency distribution from NLTK for each article, assemble the training set for a Naive Bayes classifier in the correct format, train the classifier, and return the trained classifier."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"0YbN1Adx6Zng","nbgrader":{"cell_type":"code","checksum":"cf80b7cdea908493b4167e34121f6700","grade":false,"grade_id":"2-3-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Complete the build_classifier function\n","\n","def build_classifier(positive_df, negative_df, vocabulary=[]):\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"xT6VZHwsRIfB","nbgrader":{"cell_type":"code","checksum":"54ad188d00186fa78cee7fe25838015f","grade":true,"grade_id":"2-3-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.3 for training the classifier\n","\n","classifier = build_classifier(pages_df, negative_df)\n","print(type(classifier))\n","\n","# This should print <class 'nltk.classify.naivebayes.NaiveBayesClassifier'>"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"Y3q7WyM7RVbW","nbgrader":{"cell_type":"markdown","checksum":"4e2c6be6bbb8f4d040730fd6b9d9d4dd","grade":false,"grade_id":"cell-5805e1cdf30c9b1c","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 2.4: Run the classifier\n","\n","Below are some sample pages.  Let's see if you can run the model on them."]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"senOL6iQHkim","nbgrader":{"cell_type":"markdown","checksum":"bea797c4c8a6db2d719817b7f16f3f26","grade":false,"grade_id":"cell-f311584c5d0bf3ea","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 2.4.1 Load the test set\n","\n","Adapt the code from Task 2.1 for the new dataset. Call the final dataframe `inference_df`."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"nVe0c3fHUK_x","nbgrader":{"cell_type":"code","checksum":"106f80cc575d61741b6936ebffd833af","grade":false,"grade_id":"2-4-1-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Create inference_df\n","\n","test = [\n","    'https://fried.com/history-of-bitcoin/',\n","    'https://news.wharton.upenn.edu/press-releases/2018/06/penn-launches-strategic-collaboration-ripple-accelerate-innovation-blockchain-cryptocurrency/',\n","    'https://en.wikipedia.org/wiki/Euro',\n","    'https://ew.com/movies/star-wars-rise-of-skywalker-footage-d23-expo/',\n","    'https://en.wikipedia.org/wiki/Donald_Trump'\n","]\n","\n","current_prefix = 'https://raw.githubusercontent.com/odpi/OpenDS4All/penn-processing-zgi/assets/data/scripts-autoload-data/data-wrangling/'\n","\n","rebase_test = [current_prefix + (currentpage[currentpage.find('//')+2 :  -1 if(currentpage[-1]=='/') else None]) for currentpage in test ]\n","\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"qYm4ecTHReP7","nbgrader":{"cell_type":"code","checksum":"5850ebe51ab457bc0c583995d08cb26a","grade":true,"grade_id":"2-4-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.4.1 loading the test set\n","\n","display(inference_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"MxbQZplPR4Wg","nbgrader":{"cell_type":"markdown","checksum":"3d196a152ba322f3aad165163dd0857b","grade":false,"grade_id":"cell-3b2c38ea6c280a25","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Task 2.4.2: Inference\n","\n","Now let's run your classifier over your individual documents. Adapt the code from the NLTK lecture notebook. The function classify should take as input a two column dataframe as we have made previously, the trained classifier, and an optional vocabulary list. It should return a list of booleans. For example, a perfect classifier should return\n","\n","`classify(inference_df, classifier) = [True, True, False, False, False]`.\n","\n","Note that you will need to run `get_words` (passing the vocabulary) and then generate an NLTK frequency distribution for each test article."]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"lVEaAr4e6miy","nbgrader":{"cell_type":"code","checksum":"42bda82c2761bed25308404f6892b0ff","grade":false,"grade_id":"2-4-2-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Complete the classify function\n","\n","def classify(df, classifier, vocabulary=[]):\n","# YOUR CODE HERE\n","\n","\n","results = classify(inference_df, classifier)\n","display(results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"1hlVt_58Q-mX","nbgrader":{"cell_type":"code","checksum":"faf46a0f3b717f482f24e7c4a83ef4de","grade":true,"grade_id":"2-4-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.4.2 classifier results\n","\n","if len(results) != 5:\n","    raise AssertionError('We do not have a classification for each item.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"YNo9zZLb8WTY","nbgrader":{"cell_type":"markdown","checksum":"8a426f8b0fcc42d86f54dedc6fe449a2","grade":false,"grade_id":"cell-14b50a9757ed81bd","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Task 2.5: Make the vocabulary and re-classify\n","\n","So far, our classifier is not very good. This is because it is trying to consider too many words, many of which did or did not occur in the training articles purely by chance. If we restrict the \"attention\" of the classifier to the most frequent words, it is much more likely to pick up real patterns rather than memorize accidents. We do this by making a vocabulary.\n","\n","Complete the `make_vocabulary` function below. This function should take as input the two column dataframes `positive_df` and `negative_df`, and also an integer `num`. For the positive dataframe, run `get_words` on each article (without vocabulary), concatenate all of these lists of words together, create an NLTK frequency distribution, and then finally store a list of the `num` most frequent words. Do the same for the negative dataframe. The function should return the `num` most frequent positive words and the `num` most frequent negative words concatenated into one list (2 times `num` words in all).\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"mOU-YpBE9u4a","nbgrader":{"cell_type":"code","checksum":"bb49f0466f3b33ad3312c230f37c2a59","grade":false,"grade_id":"2-5-ans","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{}},"source":["# TODO: Complete the make_vocabulary function\n","\n","def make_vocabulary(positive_df, negative_df, num):\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"Quri9yZn_fgv","nbgrader":{"cell_type":"code","checksum":"1363795adb36e655a7ab9b635e66077c","grade":true,"grade_id":"2-5-1-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.5.1 see final vocabulary size\n","\n","vocabulary = make_vocabulary(pages_df, negative_df, 30)\n","print(len(vocabulary))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"eBjSQCfV_f7z","nbgrader":{"cell_type":"code","checksum":"0eefc8d89f5ca392f2b67c52473b336f","grade":true,"grade_id":"2-5-2-sanity","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity check 2.5 improved classifier results\n","\n","classifier_with_vocab = build_classifier(pages_df, negative_df, vocabulary)\n","results = classify(inference_df, classifier_with_vocab, vocabulary)\n","display(results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ei6dMrbhklkW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}